#!/usr/bin/env python
import sys

sys.path.append('../.')
sys.path.append('.')

import os
import os.path as path
import getpass
import glob
import optparse
import socket
import shutil
import time
import numpy as np
import h5py
import vigra
import itertools
import copy
# from empryonic import track
import pgmlink as track
from empryonic import io
from multiprocessing import Pool

region_features = []

def get_feature(h5file, feature_path, object_id):
    if not feature_path in h5file:
        raise Exception("Feature %s not found in %s." % (feature_path, h5file.filename))
    return h5file[feature_path][object_id, ...]


def get_feature_vector(traxel, feature_name, num_dimensions):
    #print traxel.print_available_features()
    result = []
    for i in range(num_dimensions):
        try:
            result.append(traxel.get_feature_value(feature_name, i))
        except:
            print("Error when accessing feature {}[{}] for traxel (Id={},Timestep={})".format(feature_name,
                                                                                              i,
                                                                                              traxel.Id,
                                                                                              traxel.Timestep))
            raise Exception
    return result


def extract_coordinates(coordinate_map, h5file, traxel, options):
    # add coordinate lists with armadillo matrixes
    shape = h5file['/'.join(options.label_img_path.split('/')[:-1])].values()[0].shape[1:4]
    ndim = 2 if shape[-1]==1 else 3

    # print("Extracting coordinates of potential merger: timestep {} id {}".format(traxel.Timestep, traxel.Id))

    lower = get_feature_vector(traxel, "Coord< Minimum >", ndim)
    upper = get_feature_vector(traxel, "Coord< Maximum >", ndim)

    limg_path_at = options.label_img_path % tuple([traxel.Timestep, traxel.Timestep + 1, ] + list(shape))
    roi = [0, ] * 3
    roi[0] = slice(int(lower[0]), int(upper[0] + 1))
    roi[1] = slice(int(lower[1]), int(upper[1] + 1))
    if ndim == 3:
        roi[2] = slice(int(lower[2]), int(upper[2] + 1))
    else:
        assert ndim == 2
    image_excerpt = np.array(h5file[limg_path_at][tuple([0, ] + roi + [0, ])])
    try:
        track.extract_coordinates(coordinate_map, image_excerpt.astype(np.uint32), np.array(lower).astype(np.int64),
                                traxel)
    except:
        print("Could not run extract_coordinates for traxel: Id={} Timestep={}".format(traxel.Id, traxel.Timestep))
        raise Exception


def update_merger_features(coordinate_map, h5file, merger_traxel, new_traxel_ids, raw_h5, options, fs):
    # add coordinate lists with armadillo matrixes
    shape = h5file['/'.join(options.label_img_path.split('/')[:-1])].values()[0].shape[1:4]
    ndim = 2 if shape[-1]==1 else 3

    # print("Updating label image of merger traxel: timestep {} id {}".format(merger_traxel.Timestep, merger_traxel.Id))

    lower = get_feature_vector(merger_traxel, "Coord< Minimum >", ndim)
    upper = get_feature_vector(merger_traxel, "Coord< Maximum >", ndim)

    limg_path_at = options.label_img_path % tuple([merger_traxel.Timestep, merger_traxel.Timestep + 1, ] + list(shape))
    roi = [0, ] * 3
    roi[0] = slice(int(lower[0]), int(upper[0] + 1))
    roi[1] = slice(int(lower[1]), int(upper[1] + 1))
    if ndim == 3:
        roi[2] = slice(int(lower[2]), int(upper[2] + 1))
    else:
        assert ndim == 2
    
    label_image_excerpt = np.array(h5file[limg_path_at][tuple([0, ] + roi + [0, ])])
    min_new_traxel_id = min(new_traxel_ids)
    label_image_excerpt = np.zeros(label_image_excerpt.shape, dtype=np.uint32) + (min_new_traxel_id - 1)

    try:
        for traxel_id in new_traxel_ids:
            track.update_labelimage(coordinate_map,
                                     label_image_excerpt,
                                     np.array(lower).astype(np.int64),
                                     int(merger_traxel.Timestep),
                                     int(traxel_id))
        label_image_excerpt -= min_new_traxel_id - 1

        # Todo: store updated image somewhere?

        # compute features for new traxels, which are automatically stored in featurestore
        # the 2d raw sequence is usually saved as a 2d+t+c dataset, the labelimage from ilastik as 3d+t+c
        raw_image_excerpt = np.array(raw_h5[options.raw_path][tuple([timestep, ] + roi[0:ndim] + [0, ])])
        track.extract_region_features_roi(raw_image_excerpt.squeeze().astype(np.float32),
                                          label_image_excerpt.astype(np.uint32),
                                          np.array(range(len(new_traxel_ids) + 1)).astype(np.int64),
                                          min_new_traxel_id - 1,
                                          np.array(lower).astype(np.int64),
                                          fs,
                                          merger_traxel.Timestep)
    except MemoryError:
        print("WARNING: Something went wrong when updating label image or extracting region features for traxel "
              "(id={}, timestep={}). Features are left empty".format(merger_traxel.Id, merger_traxel.Timestep))


def generate_traxelstore(h5file,
                         options,
                         feature_path,
                         time_range,
                         x_range,
                         y_range,
                         z_range,
                         size_range,
                         x_scale=1.0,
                         y_scale=1.0,
                         z_scale=1.0,
                         with_div=True,
                         with_local_centers=False,
                         median_object_size=None,
                         max_traxel_id_at=None,
                         with_merger_prior=True,
                         max_num_mergers=1,
                         with_optical_correction=False,
                         ext_probs=None
):
    print "generating traxels"
    print "filling traxelstore"
    ts = track.TraxelStore()
    fs = track.FeatureStore()
    max_traxel_id_at = track.VectorOfInt()

    print "fetching region features and division probabilities"
    print h5file.filename, feature_path

    if with_div:
        divProbs = h5file[options.div_prob_path]

    if with_merger_prior:
        detProbs = h5file[options.obj_count_path]

    if with_local_centers:
        localCenters = None  # self.RegionLocalCenters(time_range).wait()

    if options.perturbation_distribution == "ClassifierUncertainty":
        detProb_Vars = h5file[options.obj_count_var_path]
        if with_div:
            divProb_Vars = h5file[options.div_prob_var_path]

    if x_range is None:
        x_range = [0, sys.maxint]

    if y_range is None:
        y_range = [0, sys.maxint]

    if z_range is None:
        z_range = [0, sys.maxint]

    shape_t = len(h5file[options.obj_count_path].keys())
    keys_sorted = range(shape_t)

    if time_range is not None:
        keys_sorted = [key for key in keys_sorted if int(key) >= time_range[0] and int(key) < time_range[1]]
    else:
        time_range = (0, shape_t)

    filtered_labels = {}
    obj_sizes = []
    total_count = 0
    empty_frame = False

    for t in keys_sorted:
        feats_name = options.feats_path % (t, t + 1, 'RegionCenter')
        #region_centers = np.array(feats[t]['0']['RegionCenter'])
        region_centers = np.array(h5file[feats_name])

        feats_name = options.feats_path % (t, t + 1, 'Coord<Minimum>')
        lower = np.array(h5file[feats_name])
        feats_name = options.feats_path % (t, t + 1, 'Coord<Maximum>')
        upper = np.array(h5file[feats_name])

        if region_centers.size:
            region_centers = region_centers[1:, ...]
            lower = lower[1:, ...]
            upper = upper[1:, ...]
        if with_optical_correction:
            try:
                feats_name = options.feats_path % (t, t + 1, 'RegionCenter_corr')
                region_centers_corr = np.array(h5file[feats_name])
            except:
                raise Exception, 'cannot consider optical correction since it has not been computed'
            if region_centers_corr.size:
                region_centers_corr = region_centers_corr[1:, ...]

        feats_name = options.feats_path % (t, t + 1, 'Count')
        #pixel_count = np.array(feats[t]['0']['Count'])
        pixel_count = np.array(h5file[feats_name])
        if pixel_count.size:
            pixel_count = pixel_count[1:, ...]

        print "at timestep ", t, region_centers.shape[0], "traxels found"
        count = 0
        filtered_labels[t] = []
        for idx in range(region_centers.shape[0]):
            if len(region_centers[idx]) == 2:
                x, y = region_centers[idx]
                z = 0
            elif len(region_centers[idx]) == 3:
                x, y, z = region_centers[idx]
            else:
                raise Exception, "The RegionCenter feature must have dimensionality 2 or 3."
            size = pixel_count[idx]
            if (x < x_range[0] or x >= x_range[1] or
                        y < y_range[0] or y >= y_range[1] or
                        z < z_range[0] or z >= z_range[1] or
                        size < size_range[0] or size >= size_range[1]):
                filtered_labels[t].append(int(idx + 1))
                continue
            else:
                count += 1
            traxel = track.Traxel()
            traxel.set_feature_store(fs)
            traxel.set_x_scale(x_scale)
            traxel.set_y_scale(y_scale)
            traxel.set_z_scale(z_scale)
            traxel.Id = int(idx + 1)
            traxel.Timestep = int(t)

            traxel.add_feature_array("com", 3)
            for i, v in enumerate([x, y, z]):
                traxel.set_feature_value('com', i, float(v))

            if with_optical_correction:
                traxel.add_feature_array("com_corrected", 3)
                for i, v in enumerate(region_centers_corr[idx]):
                    traxel.set_feature_value("com_corrected", i, float(v))
                if len(region_centers_corr[idx]) == 2:
                    traxel.set_feature_value("com_corrected", 2, 0.)

            if with_div:
                traxel.add_feature_array("divProb", 1)
                prob = 0.0

                prob = float(divProbs[str(t)][idx + 1][1])
                # idx+1 because region_centers and pixel_count start from 1, divProbs starts from 0
                traxel.set_feature_value("divProb", 0, prob)

            if with_local_centers:
                raise Exception, "not yet implemented"
                traxel.add_feature_array("localCentersX", len(localCenters[t][idx + 1]))
                traxel.add_feature_array("localCentersY", len(localCenters[t][idx + 1]))
                traxel.add_feature_array("localCentersZ", len(localCenters[t][idx + 1]))
                for i, v in enumerate(localCenters[t][idx + 1]):
                    traxel.set_feature_value("localCentersX", i, float(v[0]))
                    traxel.set_feature_value("localCentersY", i, float(v[1]))
                    traxel.set_feature_value("localCentersZ", i, float(v[2]))

            if with_merger_prior and ext_probs is None:
                traxel.add_feature_array("detProb", max_num_mergers + 1)
                probs = []
                for i in range(len(detProbs[str(t)][idx + 1])):
                    probs.append(float(detProbs[str(t)][idx + 1][i]))
                probs[max_num_mergers] = sum(probs[max_num_mergers:])
                for i in range(max_num_mergers + 1):
                    traxel.set_feature_value("detProb", i, float(probs[i]))

            if options.perturbation_distribution == "ClassifierUncertainty":
                traxel.add_feature_array("detProb_Var", len(detProb_Vars[str(t)][idx+1]))
                for i in range(len(detProb_Vars[str(t)][idx+1])):
                    traxel.set_feature_value("detProb_Var", i, float(detProb_Vars[str(t)][idx+1][i]))

                if with_div:
                    traxel.add_feature_array("divProb_Var", len(divProb_Vars[str(t)][idx+1]))
                    for i in range(len(divProb_Vars[str(t)][idx+1])):
                        traxel.set_feature_value("divProb_Var", i, float(divProb_Vars[str(t)][idx+1][i]))

            elif with_merger_prior and ext_probs is not None:
                assert max_num_mergers == 1, "not implemented for max_num_mergers > 1"
                detProbFilename = ext_probs % t
                detProbGroup = h5py.File(detProbFilename, 'r')['objects/meta']
                traxel_index = np.where(detProbGroup['id'].value == traxel.Id)[0][0]
                detProbFeat = [detProbGroup['prediction_class0'].value[traxel_index],
                               detProbGroup['prediction_class1'].value[traxel_index]]
                traxel.add_feature_array("detProb", 2)
                for i in xrange(len(detProbFeat)):
                    traxel.set_feature_value("detProb", i, float(detProbFeat[i]))

            traxel.add_feature_array("count", 1)
            traxel.set_feature_value("count", 0, float(size))
            if median_object_size is not None:
                obj_sizes.append(float(size))
            ts.add(fs, traxel)

        print "at timestep ", t, count, "traxels passed filter"
        max_traxel_id_at.append(int(region_centers.shape[0]))
        if count == 0:
            empty_frame = True

        total_count += count

    # load features from raw data
    if len(options.raw_filename) > 0:
        print("Computing Features from Raw Data: {}".format(options.raw_filename))
        start_time = time.time()

        with h5py.File(options.raw_filename, 'r') as raw_h5:
            shape = h5file['/'.join(options.label_img_path.split('/')[:-1])].values()[0].shape[1:4]
            shape = (len(h5file['/'.join(options.label_img_path.split('/')[:-1])].values()),) + shape
            print("Shape is {}".format(shape))

            # loop over all frames and compute features for all traxels per frame
            for timestep in xrange(max(0, time_range[0]), min(shape[0], time_range[1])):
                print("\tFrame {}".format(timestep))
                # TODO: handle smaller FOV instead of looking at full frame
                label_image_path = options.label_img_path % (timestep, timestep+1, shape[1], shape[2], shape[3])
                label_image = np.array(h5file[label_image_path][0, ..., 0]).squeeze().astype(np.uint32)
                raw_image = np.array(raw_h5['/'.join(options.raw_path.split('/'))][timestep, ..., 0]).squeeze().astype(np.float32)
                max_traxel_id = track.extract_region_features(raw_image, label_image, fs, timestep)

                # uncomment the following if no features are taken from the ilp file any more
                #
                #max_traxel_id_at.append(max_traxel_id)
                # for idx in xrange(1, max_traxel_id):
                #     traxel = track.Traxel()
                #     traxel.set_x_scale(x_scale)
                #     traxel.set_y_scale(y_scale)
                #     traxel.set_z_scale(z_scale)
                #     traxel.Id = idx
                #     traxel.Timestep = timestep
                #     ts.add(fs, traxel)

        end_time = time.time()
        print("Feature computation for a dataset of shape {} took {} secs".format(shape, end_time - start_time))
        #fs.dump()

    if median_object_size is not None:
        median_object_size[0] = np.median(np.array(obj_sizes), overwrite_input=True)
        print 'median object size = ' + str(median_object_size[0])

    return ts, fs, max_traxel_id_at  # , filtered_labels, empty_frame


def write_detections(detections, fn):
    with io.LineageH5(fn, 'r') as f:
        traxel_ids = f['objects/meta/id'].value
        valid = f['objects/meta/valid'].value
    assert (len(traxel_ids) == len(valid))
    assert (len(detections) == len(np.flatnonzero(valid)))

    detection_indicator = np.zeros(len(traxel_ids), dtype=np.uint16)

    for i, traxel_id in enumerate(traxel_ids):
        if valid[i] != 0:
            if detections[int(traxel_id)]:
                detection_indicator[i] = 1
            else:
                detection_indicator[i] = 0

    with io.LineageH5(fn, 'r+') as f:
        # delete old dataset
        if "detection" in f['objects/meta'].keys():
            del f["objects/meta/detection"]
        f.create_dataset('objects/meta/detection', data=detection_indicator)


def write_events(events, fn):
    dis = []
    app = []
    div = []
    mov = []
    mer = []
    mul = []
    print "-- Writing results to " + path.basename(fn)
    for event in events:
        if event.type == track.EventType.Appearance:
            app.append((event.traxel_ids[0], event.energy))
        if event.type == track.EventType.Disappearance:
            dis.append((event.traxel_ids[0], event.energy))
        if event.type == track.EventType.Division:
            div.append((event.traxel_ids[0], event.traxel_ids[1], event.traxel_ids[2], event.energy))
        if event.type == track.EventType.Move:
            mov.append((event.traxel_ids[0], event.traxel_ids[1], event.energy))
        if event.type == track.EventType.Merger:
            mer.append((event.traxel_ids[0], event.traxel_ids[1], event.energy))
        if event.type == track.EventType.MultiFrameMove:
            mul.append(tuple(event.traxel_ids) + (event.energy,))

    # convert to ndarray for better indexing
    dis = np.asarray(dis)
    app = np.asarray(app)
    div = np.asarray(div)
    mov = np.asarray(mov)
    mer = np.asarray(mer)
    mul = np.asarray(mul)

    # write only if file exists
    with io.LineageH5(fn, 'r+') as f_curr:
        # delete old tracking
        if "tracking" in f_curr.keys():
            del f_curr["tracking"]

        tg = f_curr.create_group("tracking")

        # write associations
        if len(app):
            ds = tg.create_dataset("Appearances", data=app[:, :-1], dtype=np.int32)
            ds.attrs["Format"] = "cell label appeared in current file"

            ds = tg.create_dataset("Appearances-Energy", data=app[:, -1], dtype=np.double)
            ds.attrs["Format"] = "lower energy -> higher confidence"

        if len(dis):
            ds = tg.create_dataset("Disappearances", data=dis[:, :-1], dtype=np.int32)
            ds.attrs["Format"] = "cell label disappeared in current file"

            ds = tg.create_dataset("Disappearances-Energy", data=dis[:, -1], dtype=np.double)
            ds.attrs["Format"] = "lower energy -> higher confidence"

        if len(mov):
            ds = tg.create_dataset("Moves", data=mov[:, :-1], dtype=np.int32)
            ds.attrs["Format"] = "from (previous file), to (current file)"

            ds = tg.create_dataset("Moves-Energy", data=mov[:, -1], dtype=np.double)
            ds.attrs["Format"] = "lower energy -> higher confidence"

        if len(div):
            ds = tg.create_dataset("Splits", data=div[:, :-1], dtype=np.int32)
            ds.attrs["Format"] = "ancestor (previous file), descendant (current file), descendant (current file)"

            ds = tg.create_dataset("Splits-Energy", data=div[:, -1], dtype=np.double)
            ds.attrs["Format"] = "lower energy -> higher confidence"

        if len(mer):
            ds = tg.create_dataset("Mergers", data=mer[:, :-1], dtype=np.int32)
            ds.attrs["Format"] = "descendant (current file), number of objects"

            ds = tg.create_dataset("Mergers-Energy", data=mer[:, -1], dtype=np.double)
            ds.attrs["Format"] = "lower energy -> higher confidence"

        if len(mul):
            ds = tg.create_dataset("MultiFrameMoves", data=mul[:, :-1], dtype=np.int32)
            ds.attrs["Format"] = "from (given by timestep), to (current file), timestep"

            ds = tg.create_dataset("MultiFrameMoves-Energy", data=mul[:, -1], dtype=np.double)
            ds.attrs["Format"] = "lower energy -> higher confidence"

    print "-> results successfully written"


def save_events(out_dir, events, t0, t1, label_image_path, max_traxel_id_at, src_filename):
    # save events
    print "Saving events..."
    print "Length of events " + str(len(events))

    if not path.exists(out_dir):
        try:
            os.makedirs(out_dir)
        except:
            pass

    working_fns = [out_dir + "/%04d.h5" % timestep for timestep in xrange(t0, t1 + 1)]
    assert (len(events) + 1 == len(working_fns))
    with h5py.File(src_filename, 'r') as src_file:
        # first timestep without tracking
        with io.LineageH5(working_fns[0], 'w') as dest_file:
            print "-- writing empty tracking to base file", working_fns[0]
            #shape = src_file[trans_vector_path].shape
            li_name = label_image_path % (t0, t0 + 1, shape[0], shape[1], shape[2])
            label_img = np.array(src_file[li_name][0, ..., 0]).squeeze()
            seg = dest_file.create_group('segmentation')
            seg.create_dataset("labels", data=label_img)
            meta = dest_file.create_group('objects/meta')
            ids = np.unique(label_img)
            m = max_traxel_id_at[0]
            assert np.all(ids == np.arange(m + 1, dtype=ids.dtype))
            ids = ids[ids > 0]
            valid = np.ones(ids.shape)
            meta.create_dataset("id", data=ids, dtype=np.uint32)
            meta.create_dataset("valid", data=valid, dtype=np.uint32)

            # create empty tracking group
            #dest_file.create_group('tracking')
            write_events(first_events, working_fns[0])
            print "-> base file successfully written"

        # tracked timesteps
        for i, events_at in enumerate(events):
            with io.LineageH5(working_fns[i + 1], 'w') as dest_file:
                # [t0+1+i,...] or [t0+i,...]?
                #shape = src_file[trans_vector_path].shape
                li_name = label_image_path % (t0 + i + 1, t0 + i + 2, shape[0], shape[1], shape[2])
                label_img = np.array(src_file[li_name][0, ..., 0]).squeeze()
                seg = dest_file.create_group('segmentation')
                seg.create_dataset("labels", data=label_img)

                meta = dest_file.create_group('objects/meta')
                ids = np.unique(label_img)
                m = max_traxel_id_at[i + 1]
                assert np.all(ids == np.arange(m + 1, dtype=ids.dtype))
                ids = ids[ids > 0]
                valid = np.ones(ids.shape)
                meta.create_dataset("id", data=ids[::-1], dtype=np.uint32)
                meta.create_dataset("valid", data=valid, dtype=np.uint32)

            write_events(events_at, working_fns[i + 1])

def rank_solutions(ground_truth_filename, feature_vector_filename, proposal_basename, num_iterations):
    import structsvm
    proposal_filenames = [proposal_basename + '_' + str(i) + '.txt' for i in range(num_iterations)]

    problem = structsvm.struct_svm_ranking_problem(feature_vector_filename,
                                         ground_truth_filename,
                                         proposal_filenames)
                                         #,args.loss_weights)
    solver = structsvm.struct_svm_solver_primal(problem)
    weights = solver.solve()
    weights = np.array(weights)
    problem.print_scores(weights)

    print("Found weights: {}".format(weights))
    return weights

def loadGPClassifier(fn, h5_group='/TransitionGPClassifier/'):
    try:
        from lazyflow.classifiers.gaussianProcessClassifier import GaussianProcessClassifier
    except:
        raise Exception, "cannot import GP Classifier: lazyflow branch gaussianProcessClassifier must be in PYTHONPATH!"

    with h5py.File(fn, 'r') as f:
        try:
            g = f[h5_group]
        except:
            raise Exception, h5_group + " does not exist in " + fn

        gpc = GaussianProcessClassifier()
        gpc.deserialize_hdf5(g)

        features = []
        for op in g['Features'].keys():
            for feat in g['Features'][op]:
                features.append('%s<%s>' % (op, feat))

    return gpc, features

def construct_feature_vector(feature_matrix_basename,
                             proposal_basename,
                             iteration,
                             feature_vector):
    feature_matrix = np.loadtxt(feature_matrix_basename + '_{}.txt'.format(iteration))
    proposal_label = np.loadtxt(proposal_basename + '_{}.txt'.format(iteration))

    new_features = np.dot(proposal_label, feature_matrix)
    feature_vector = list(feature_vector) + list(new_features)
    return feature_vector


def getConfigAndCommandLineArguments():

    usage = """%prog [options] FILES
Track cells.

Before processing, input files are copied to OUT_DIR. Groups, that will not be modified are not
copied but linked to the original files to improve execution speed and storage requirements.
"""

    parser = optparse.OptionParser(usage=usage)
    parser.add_option('--config-file', type='string', dest='config', default=None,
                      help='path to config file')
    parser.add_option('--method', type='str', default='conservation',
                      help='chaingraph or conservation [default: %default]')
    parser.add_option('-o', '--output-dir', type='str', dest='out_dir', default='tracked', help='[default: %default]')
    parser.add_option('--x-scale', type='float', dest='x_scale', default=1., help='[default: %default]')
    parser.add_option('--y-scale', type='float', dest='y_scale', default=1., help='[default: %default]')
    parser.add_option('--z-scale', type='float', dest='z_scale', default=1., help='[default: %default]')
    parser.add_option('--with-visbricks', action='store_true', dest='with_visbricks',
                      help='writze out a "time dependent HDF5" file for visbricks')
    parser.add_option('--with-rel-linking', action='store_true', dest='with_rel_linking',
                      help='link hdf5 files relative instead of absolute')
    parser.add_option('--full-copy', action='store_true', dest='full_copy',
                      help='do not link to but copy input files completely')
    parser.add_option('--user', type='str', dest='user', default=getpass.getuser(),
                      help='user to log [default: %default]')
    parser.add_option('--date', type='str', dest='date', default=time.ctime(),
                      help='datetime to log [default: %default]')
    parser.add_option('--machine', type='str', dest='machine', default=socket.gethostname(),
                      help='machine to log [default: %default]')
    parser.add_option('--comment', type='str', dest='comment', default='none',
                      help='some comment to log [default: %default]')
    parser.add_option('--random-forest', type='string', dest='rf_fn', default=None,
                      help='use cellness prediction instead of indicator function for (mis-)detection energy')
    parser.add_option('--ep_gap', type='float', dest='ep_gap', default=0.01,
                      help='stop optimization as soon as a feasible integer solution is found proved to be within the given percent of the optimal solution')
    parser.add_option('-f', '--forbidden_cost', type='float', dest='forbidden_cost', default=0,
                      help='forbidden cost [default: %default]')
    parser.add_option('--min-ts', type='int', dest='mints', default=0, help='[default: %default]')
    parser.add_option('--max-ts', type='int', dest='maxts', default=-1, help='[default: %default]')
    parser.add_option('--min-size', type='int', dest='minsize', default=0,
                      help='minimal size of objects to be tracked [default: %default]')
    parser.add_option('--dump-traxelstore', type='string', dest='dump_traxelstore', default=None,
                      help='dump traxelstore to file [default: %default]')
    parser.add_option('--load-traxelstore', type='string', dest='load_traxelstore', default=None,
                      help='load traxelstore from file [default: %default]')
    parser.add_option('--raw-data-file', type='string', dest='raw_filename', default=None,
                      help='filename to the raw h5 file')
    parser.add_option('--raw-data-path', type='string', dest='raw_path', default='volume/data',
                      help='Path inside the raw h5 file to the data')

    #funkey learning parameters
    parser.add_option('--export-funkey-files', action='store_true', dest='eff', default=False,
                    help='export labels features and constraints to learn with funkey')
    parser.add_option('--learn-perturbation-weights', action='store_true', dest='learn_perturbation_weights', default=False,
                    help='learn free parameters of perturbation models')
    parser.add_option('--without-classifier', action='store_true', dest='woc', default=False,
                    help='use classifier for labeling')
    parser.add_option('--gt-path', type='string', dest='gt_pth', default=".", help='')
    parser.add_option('--funkey-learn', action='store_true', dest='learn_funkey', default=False,
                    help='export labels features and constraints to learn with funkey')
    parser.add_option('--funkey-weights', type='string', dest='funkey_weights', default="",
                    help='weights for weighted hamming loss')
    parser.add_option('--funkey-regularizerWeight', type='float', dest='funkey_regularizerWeight', default=1,
                    help='stop optimization as soon as a feasible integer solution is found proved to be within the given percent of the optimal solution')
    parser.add_option('--compare-script-path', type='string', dest='compare_path')


    chaingraphopts = optparse.OptionGroup(parser, "chaingraph method")
    chaingraphopts.add_option('-p', '--opportunity_cost', type='float', dest='opp', default=0,
                              help='opportunity cost [default: %default]')

    chaingraphopts.add_option('--without-hard-constraints', action='store_true', dest='without_constraints',
                              help='without hard constraints')
    chaingraphopts.add_option('--fixed-detections', action='store_true', dest='fixed_detections',
                              help='set all detections to active via hard constraint; not affected by --without-hard-constraints')
    chaingraphopts.add_option('--mean_div_dist', type='float', dest='mdd', default=25,
                              help='average division distance [default: %default]')
    chaingraphopts.add_option('--min_angle', type='float', dest='ma', default=0,
                              help='minimal division angle [default: %default]')
    chaingraphopts.add_option('-a', '--appearance', type='float', dest='app', default=500,
                              help='appearance cost [default: %default]')
    chaingraphopts.add_option('-d', '--disappearance', type='float', dest='dis', default=500,
                              help='disappearance cost [default: %default]')
    chaingraphopts.add_option('-e', '--detection', type='float', dest='det', default=10,
                              help='detection weight [default: %default]')
    chaingraphopts.add_option('-m', '--misdetection', type='float', dest='mis', default=200,
                              help='misdetection weight [default: %default]')
    chaingraphopts.add_option('-n', '--n-neighbors', type='int', dest='nnc', default=2,
                              help='number of neighbors taken into account when building hypothesesgraph [default: %default]')

    consopts = optparse.OptionGroup(parser, "conservation tracking")
    consopts.add_option('--max-number-objects', dest='max_num_objects', type='float', default=2,
                        help='Give maximum number of objects one connected component may consist of [default: %default]')
    consopts.add_option('--max-neighbor-distance', dest='mnd', type='float', default=30, help='[default: %default]')
    consopts.add_option('--division-threshold', dest='division_threshold', type='float', default=0.1, help='[default: %default]')
    # detection_rf_filename in general parser options
    consopts.add_option('--size-dependent-detection-prob', dest='size_dependent_detection_prob', action='store_true')
    # forbidden_cost in general parser options
    # ep_gap in general parser options
    consopts.add_option('--average-obj-size', dest='aos', type='float', default=0, help='[default: %default]')
    consopts.add_option('--without-tracklets', dest='without_tracklets', action='store_true')
    consopts.add_option('--with-opt-correct', dest='woptical', action='store_true')
    consopts.add_option('--det', dest='detection_weight', type='float', default=10.0, help='detection weight [default: %default]')
    consopts.add_option('--div', dest='division_weight', type='float', default=10.0, help='division weight [default: %default]')
    consopts.add_option('--dis', dest='disappearance_cost', type='float', default=500.0, help='disappearance cost [default: %default]')
    consopts.add_option('--app', dest='appearance_cost', type='float', default=500.0, help='appearance cost [default: %default]')
    consopts.add_option('--tr', dest='transition_weight', type='float', default=10.0, help='transition weight [default: %default]')
    consopts.add_option('--without-divisions', dest='without_divisions', action='store_true')
    consopts.add_option('--means', dest='means', type='float', default=0.0,
                        help='means for detection [default: %default]')
    consopts.add_option('--sigma', dest='sigma', type='float', default=0.0,
                        help='sigma for detection [default: %default]')
    consopts.add_option('--with-merger-resolution', dest='with_merger_resolution', action='store_true', default=False)
    consopts.add_option('--without-constraints', dest='woconstr', action='store_true', default=False)
    consopts.add_option('--trans-par', dest='trans_par', type='float', default=5.0,
                        help='alpha for the transition prior [default: %default]')
    consopts.add_option('--border-width', dest='border_width', type='float', default=0.0,
                        help='absolute border margin in which the appearance/disappearance costs are linearly decreased [default: %default]')
    consopts.add_option('--ext-probs', dest='ext_probs', type='string', default=None,
                        help='provide a path to hdf5 files containing detection probabilities [default:%default]')
    consopts.add_option('--objCountPath', dest='obj_count_path', type='string',
                        default='/CellClassification/Probabilities/0/',
                        help='internal hdf5 path to object count probabilities [default=%default]')
    consopts.add_option('--divPath', dest='div_prob_path', type='string', default='/DivisionDetection/Probabilities/0/',
                        help='internal hdf5 path to division probabilities [default=%default]')
    consopts.add_option('--featsPath', dest='feats_path', type='string',
                        default='/ObjectExtraction/RegionFeatures/0/[[%d], [%d]]/Default features/%s',
                        help='internal hdf5 path to object features [default=%default]')
    consopts.add_option('--translationPath', dest='trans_vector_path', type='str',
                        default='OpticalTranslation/TranslationVectors/0/data',
                        help='internal hdf5 path to translation vectors [default=%default]')
    consopts.add_option('--labelImgPath', dest='label_img_path', type='str',
                        default='/ObjectExtraction/LabelImage/0/[[%d, 0, 0, 0, 0], [%d, %d, %d, %d, 1]]',
                        help='internal hdf5 path to label image [default=%default]')
    consopts.add_option('--timeout', dest='timeout', type='float', default=1e+75, help='CPLEX timeout in sec. [default: %default]')

    # options for perturbing and re-ranking
    consopts.add_option('--num-iterations', dest='num_iterations', type='int', default=1,
                        help='number of iterations to perform inference, >1 needed for uncertainty [default=%default]')
    consopts.add_option('--perturb-distrib', dest='perturbation_distribution', type='str', default='DiverseMbest',
                        help='distribution type for parameter perturbation, one of {GaussianPertubation, PerturbAndMAP, '
                             'DiverseMbest, MbestCPLEX, ClassifierUncertainty} [default=%default]')
    consopts.add_option('--perturb-sigmas', dest='perturb_sigmas', type=float, nargs=5, default=[0., 0., 10., 10., 10.],
                        help='Parameters for the perturbation distribution. [default=%default]')
    consopts.add_option('--objCountVarPath', dest='obj_count_var_path', type='string',
                        default='/CountClassification/Uncertainty/0/',
                        help='internal path to count classification variance [default=%default]')
    consopts.add_option('--divVarPath', dest='div_prob_var_path', type='string',
                        default='/DivisionDetection/Uncertainty/0/',
                        help='internal path to division detection variance [default=%default]')
    consopts.add_option('--transitionClassifierFn', dest='trans_fn', type='string',
                        default=None, help='filename of transition classifier, empty string for None [default=%default]')
    consopts.add_option('--transitionClassifierPath', dest='trans_path', type='string',
                        default='/TransitionClassifier', help='internal h5 path to transition classifier [default=%default]')

    # learning options
    consopts.add_option('--no-saving', dest='skip_saving', action='store_true',
                        help='Skip saving if you are just interested in the proposals and features')
    consopts.add_option('--ground-truth-labeling', dest='ground_truth_labeling', type='str', default='',
                        help='Ground Truth label filename which will be used for reranking the solutions')
    consopts.add_option('--gt-label-image-path', dest='gt_label_image_path', type='str', default='',
                        help='Internal path to the label image inside the ground truth HDF5')
    consopts.add_option('--save-outlier-svm', dest='save_outlier_svm', type='str', default='',
                        help='Filename to store the trained SVM for higher order features outlier detection. Requires gt-path to be set')
    consopts.add_option('--load-outlier-svm', dest='load_outlier_svm', type='str', default='',
                        help='Load a trained outlier svm from disk')
    consopts.add_option('--reranker-weight-file', dest='reranker_weight_file', type='str', default='',
                        help='File in which the reranker stored the weights after learning')

    parser.add_option_group(chaingraphopts)
    parser.add_option_group(consopts)

    optcfg, args = parser.parse_args()

    configfilecommands = []

    if(optcfg.config != None):
        with open(optcfg.config) as f:
            configfilecommands = f.read().splitlines()
        optcfg, args2 = parser.parse_args(configfilecommands)

    print('--------------')
    for key, value in sorted(vars(optcfg).items()):
        print('{} : {}'.format(key, value))
    print('--------------')

    numArgs = len(args)
    if numArgs == 0:
        parser.print_help()
        sys.exit(1)

    return optcfg, args


def store_label_in_hypotheses_graph():
    graph = tracker.buildGraph(ts)
    # label full graph
    n_it = track.NodeIt(graph)
    a_it = track.ArcIt(graph)
    tmap = graph.getNodeTraxelMap()
    nodeTimestepIdMap = {}
    arcTimestepIdMap = {}
    for n in n_it:
        #print tmap[n].Timestep,tmap[n].Id
        nodeTimestepIdMap[tmap[n].Timestep, tmap[n].Id] = n
        graph.addAppearanceLabel(n, 0)
        graph.addDisappearanceLabel(n, 0)
        graph.addDivisionLabel(n, 0)
    for a in a_it:
        #print tmap[g.source(a)].Timestep,tmap[g.source(a)].Id,tmap[g.target(a)].Id
        graph.addArcLabel(a, 0)
        arcTimestepIdMap[tmap[graph.source(a)].Timestep, tmap[graph.source(a)].Id, tmap[graph.target(a)].Id] = a


        #load ground truth from hd5 files and label graph accordingly
    for i in range(graph.earliest_timestep(), graph.latest_timestep() + 1):
        print "open file:", options.gt_pth.rstrip('/') + '/' + "{0:04d}".format(i) + '.h5'
        f = h5py.File(options.gt_pth.rstrip('/') + '/' + "{0:04d}".format(i) + '.h5', 'r')
        print f

        try:
            applist = f["tracking"]["Appearances"]
        except:
            applist = []

        try:
            dislist = f["tracking"]["Disappearances"]
        except:
            dislist = []

        try:
            movlist = f["tracking"]["Moves"]
            print movlist
        except:
            movlist = []

        try:
            spllist = f["tracking"]["Splits"]
        except:
            spllist = []

        try:
            merlist = f["tracking"]["Mergers"]
        except:
            merlist = []

        mdic = {}
        for m in merlist:
            mdic[m[0]] = np.asscalar(m[1])

        for appset in applist:
            if((i,appset[0]) in nodeTimestepIdMap):
                graph.addAppearanceLabel(nodeTimestepIdMap[i,appset[0]],mdic.get(appset[0],1))
                graph.addDisappearanceLabel(nodeTimestepIdMap[i,appset[0]],0)
            else:
                print "ERROR IN app",i,appset[0]
        for disset in dislist:
            if((i-1,disset[0]) in nodeTimestepIdMap):
                graph.addAppearanceLabel(nodeTimestepIdMap[i-1,disset[0]],0)
                graph.addDisappearanceLabel(nodeTimestepIdMap[i-1,disset[0]],mdic.get(disset[0],1))
            else:
                print "ERROR IN disapp",i-1,disset[0]
        for movset in movlist:
            if((i-1,movset[0]) in nodeTimestepIdMap and (i,movset[1])in nodeTimestepIdMap):
                graph.addAppearanceLabel(nodeTimestepIdMap[i-1,movset[0]],mdic.get(movset[0],1))
                graph.addDisappearanceLabel(nodeTimestepIdMap[i,movset[1]],mdic.get(movset[1],1))
                if((i-1,movset[0],movset[1]) in arcTimestepIdMap):
                    graph.addArcLabel(arcTimestepIdMap[i-1,movset[0],movset[1]],1)
                else:
                    print "ERROR3 IN move",i-1,movset[0],movset[1],"  adding new arc"
                    newarc = graph.addArc(nodeTimestepIdMap[i-1,movset[0]],nodeTimestepIdMap[i,movset[0]])
                    graph.addArcLabel(newarc,1)

        for splset in spllist:
            if((i-1,splset[0],splset[1]) in arcTimestepIdMap and (i-1,splset[0],splset[2]) in arcTimestepIdMap):
                graph.addArcLabel(arcTimestepIdMap[i-1,splset[0],splset[1]],1)
                graph.addArcLabel(arcTimestepIdMap[i-1,splset[0],splset[2]],1)
            else:
                print "ERROR IN SPLIT arc", i-1,splset[0],splset[2]
            if((i-1,splset[0]) in nodeTimestepIdMap and (i,splset[1]) in nodeTimestepIdMap and (i,splset[2]) in nodeTimestepIdMap ):
                graph.addDivisionLabel(nodeTimestepIdMap[i-1,splset[0]],1)
                graph.addAppearanceLabel(nodeTimestepIdMap[i,splset[1]],mdic.get(splset[1],1))
                graph.addAppearanceLabel(nodeTimestepIdMap[i,splset[2]],mdic.get(splset[2],1))
                graph.addDisappearanceLabel(nodeTimestepIdMap[i-1,splset[0]],1)
            else:
                print "ERROR IN SPLIT node",i-1,splset[0],splset[2]

    return graph


def train_outlier_svm(options):
    if len(options.save_outlier_svm) > 0 and len(options.gt_pth) > 0:
        print("Storing ground truth labels in hypotheses graph and training an outlier detector SVM from that")
        g = store_label_in_hypotheses_graph()
        print("Setting injected solution as active")
        g.set_injected_solution()
        feature_extractor = track.TrackingFeatureExtractor(g, fov)
        print("Train svm")
        feature_extractor.train_track_svm()
        print("Done training, saving...")
        outlier_svm = feature_extractor.get_track_svm()
        with open(options.save_outlier_svm, 'w') as svm_dump:
            pickle.dump(outlier_svm, svm_dump)
        print("SVM is trained and stored in " + options.save_outlier_svm)
        options.load_outlier_svm = options.save_outlier_svm


def get_track_feature_matrix(track_features_filename, normalize=False):
    num_features = sum([i for n,i in region_features])
    track_feature_matrix = np.array([]).reshape(0, num_features)
    with h5py.File(track_features_filename, 'r') as track_features:
        # examine all tracks
        for track_id in track_features['tracks'].keys():
            merged_features = np.zeros(num_features)

            try:
                # get accumulated feature vector
                traxels = np.array(track_features['tracks/' + track_id + '/traxels']).transpose()
                for timestep, idx in traxels:
                    feats = []
                    traxel = ts.get_traxel(int(idx), int(timestep))
                    for feat_name, feat_dims in region_features:
                        feats += get_feature_vector(traxel, feat_name, feat_dims)

                    merged_features += np.array(feats)
            except:
                print("Warning: could not extract some feature of track {}, setting it's feature value to zero")

            # divide by num traxels to get average
            if normalize:
                merged_features = [f / len(traxels) for f in merged_features]

            track_feature_matrix = np.vstack([track_feature_matrix, merged_features])
    return track_feature_matrix

def extract_features_and_compute_score(options, i, hypotheses_graph, ts, fov, feature_vector_filename):
    # extract higher order features and per-track features
    print("Extracting features of solution {}".format(i))
    track_features_filename = options.out_dir.rstrip('/') + '/iter_' + str(i) + '/track_features.h5'
    feature_extractor = track.TrackingFeatureExtractor(hypotheses_graph, fov)
    if len(options.load_outlier_svm) > 0:  # when the svm was trained in this run, it automatically sets load_outlier_svm
        with open(options.save_outlier_svm, 'r') as svm_dump:
            outlier_svm = pickle.load(svm_dump)
            feature_extractor.set_track_svm(outlier_svm)
            print("Trained outlier SVM loaded from " + options.load_outlier_svm)
    feature_extractor.set_track_feature_output_file(track_features_filename)
    feature_extractor.compute_features()
    feature_vector = [f for f in feature_extractor.get_feature_vector()]

    track_feature_matrix = get_track_feature_matrix(track_features_filename, False)
    sum_of_track_features = np.sum(track_feature_matrix, axis=0)

    feature_vector += list(sum_of_track_features)
    overall_score = 0

    # if reranker weights are already given, compute overall, and track scores
    if options.reranker_weight_file and len(options.reranker_weight_file) > 0:
        reranker_weights = np.loadtxt(options.reranker_weight_file)
        overall_score = np.dot(reranker_weights, np.array(feature_vector))

        # track scores
        track_feature_matrix = get_track_feature_matrix(track_features_filename, True)
        with h5py.File(track_features_filename, 'r') as track_features:
            track_scores = []
            # examine all tracks, extract HO features for quality measures
            for track_id in track_features['tracks'].keys():

                # ugly way of using the learned weights to compute track quality measurements
                score = 0

                def get_feature_score(mean_weight_idx, name):
                    try:
                        value = np.array(track_features['tracks/' + track_id + '/' + name])
                        return reranker_weights[mean_weight_idx] * np.mean(value) \
                               + reranker_weights[mean_weight_idx + 1] * np.var(value)
                    except:
                        return 0

                score += get_feature_score(0, 'sq_diff_RegionCenter')
                score += get_feature_score(4, 'sq_diff_Count')
                score += get_feature_score(8, 'sq_diff_Mean')
                score += get_feature_score(12, 'sq_diff_Variance')
                score += get_feature_score(16, 'sq_accel_RegionCenter')
                score += get_feature_score(20, 'sq_accel_Count')
                score += get_feature_score(24, 'sq_accel_Mean')
                score += get_feature_score(28, 'sq_accel_Variance')
                score += get_feature_score(32, 'angles_RegionCenter')
                score += get_feature_score(38, 'outlier_id_RegionCenter')
                score += get_feature_score(42, 'outlier_id_Count')
                score += get_feature_score(46, 'outlier_id_Mean')
                score += get_feature_score(50, 'outlier_id_Variance')
                score += get_feature_score(54, 'diff_outlier_RegionCenter')
                score += get_feature_score(58, 'diff_outlier_Count')
                score += get_feature_score(62, 'diff_outlier_Mean')
                score += get_feature_score(66, 'diff_outlier_Variance')

                track_scores.append([float(track_id), score])

            #Todo: analyze all divisions

            #Todo: stitch tracks (match last track traxel id to first division traxel id and vice versa)
            #       and then compute quality measures
            track_scores = np.array(track_scores)
            print("Saving track scores for iteration " + str(i))
            np.savetxt(options.out_dir.rstrip('/') + '/iter_' + str(i) + '/track_scores.txt', track_scores)

            import matplotlib.pyplot as plt
            plt.figure()
            plt.hist(track_scores[:,1], 100)
            plt.savefig(options.out_dir.rstrip('/') + '/iter_' + str(i) + '/track_scores.pdf')

        print("Overall score of tracking is: {}".format(score))

    return feature_vector, overall_score

if __name__ == "__main__":

    options, args = getConfigAndCommandLineArguments()

    numArgs = len(args)
    if numArgs > 0:
        fns = []
        for arg in args:
            print arg
            fns.extend(glob.glob(arg))
        fns.sort()
        print(fns)

    ### Do the tracking
    start = time.time()

    feature_path = options.feats_path
    with_div = True
    with_merger_prior = True
    if options.method == 'chaingraph':
        feature_path = 'ObjectExtraction'
        with_merger_prior = True  # the chaingraph will use the same random forest for detection/misdetection
    # read all traxels into TraxelStore
    print fns[0]
    time_range = [options.mints, options.maxts]
    if options.maxts == -1:
        time_range = None
    obj_size = [0]
    max_traxel_id_at = []

    if options.trans_fn is None or len(options.trans_fn) == 0:
        trans_classifier = None
    else:
        try:
            from lazyflow.classifiers.TransitionClassifier import TransitionClassifier
        except:
            print("Pythonpath: {}".format(sys.path))
            raise Exception, "cannot import Transition Classifier: lazyflow branch gaussianProcessClassifier must be in PYTHONPATH!"

        print 'load pre-trained transition classifier'
        gpc, selected_features = loadGPClassifier(fn=options.trans_fn, h5_group=options.trans_path)
        trans_classifier = TransitionClassifier(gpc, selected_features)

    with h5py.File(fns[0], 'r') as h5file:
        ndim = 3

        if h5file['/'.join(options.label_img_path.strip('/').split('/')[:-1])].values()[0].shape[3] == 1:
            ndim = 2
        print 'ndim=', ndim

        region_features = [
            ("RegionCenter", ndim),
            ("Count", 1),
            ("Variance", 1),
            ("Sum", 1),
            ("Mean", 1),
            ("RegionRadii", ndim),
            ("Central< PowerSum<2> >", 1),
            ("Central< PowerSum<3> >", 1),
            ("Central< PowerSum<4> >", 1),
            ("Kurtosis", 1),
            ("Maximum", 1),
            ("Minimum", 1),
            ("RegionAxes", ndim**2),
            ("Skewness", 1),
            ("Weighted<PowerSum<0> >", 1),
            ("Coord< Minimum >", ndim),
            ("Coord< Maximum >", ndim)
        ]

        if options.load_traxelstore:
            print 'loading traxelstore from file'
            import pickle

            with open(options.load_traxelstore, 'rb') as ts_in:
                ts = pickle.load(ts_in)
                fs = pickle.load(ts_in)
                max_traxel_id_at = pickle.load(ts_in)
                ts.set_feature_store(fs)
        else:
            if options.method == 'conservation':
                max_num_mer = int(options.max_num_objects)
            else:
                max_num_mer = 1
            ts, fs, max_traxel_id_at = generate_traxelstore(h5file=h5file,
                                                            options=options,
                                                            feature_path=feature_path,
                                                            time_range=time_range,
                                                            x_range=None,
                                                            y_range=None,
                                                            z_range=None,
                                                            size_range=[options.minsize, 10000],
                                                            x_scale=options.x_scale,
                                                            y_scale=options.y_scale,
                                                            z_scale=options.z_scale,
                                                            with_div=with_div,
                                                            with_local_centers=False,
                                                            median_object_size=obj_size,
                                                            max_traxel_id_at=max_traxel_id_at,
                                                            with_merger_prior=with_merger_prior,
                                                            max_num_mergers=max_num_mer,
                                                            with_optical_correction=bool(options.woptical),
                                                            ext_probs=options.ext_probs
            )

        if options.dump_traxelstore:
            print 'dumping traxelstore to file'
            import pickle

            with open(options.dump_traxelstore, 'wb') as ts_out:
                pickle.dump(ts, ts_out)
                pickle.dump(fs, ts_out)
                pickle.dump(max_traxel_id_at, ts_out)

        """for i, fn in enumerate(working_fns):
            print "-- reading Traxels from " + fn
            f = io.LineageH5(fn, 'r', timestep=i)
            f.x_scale = options.x_scale
            f.y_scale = options.y_scale
            f.z_scale = options.z_scale
            traxels = f.cTraxels()
            ts.add_from_Traxels(traxels)
            f.close()
            del f
            print "-> %d traxels read" % len(traxels)"""

    if options.aos != 0:
        obj_size[0] = options.aos

    info = [int(x) for x in ts.bounding_box()]
    if info[0] != options.mints or info[4] != options.maxts - 1:
        print("Warning: Traxelstore has different time range than requested. Using full traxelstore range.")
    t0, t1 = (info[0], info[4])
    print "-> Traxelstore bounding box: " + str(info)

    print "Start tracking..."
    if (options.method == "conservation"):
        rf_fn = 'none'
        if options.rf_fn:
            rf_fn = options.rf_fn
        with h5py.File(fns[0], 'r') as h5file:
            shape = h5file['/'.join(options.label_img_path.split('/')[:-1])].values()[0].shape[1:4]
            [xshape, yshape, zshape] = shape
        fov = track.FieldOfView(t0, 0, 0, 0, t1, options.x_scale * (xshape - 1), options.y_scale * (yshape - 1),
                                options.z_scale * (zshape - 1))
        if ndim == 2:
            assert options.z_scale * (zshape - 1) == 0, "fov of z must be (0,0) if ndim == 2"

        #tracker = track.ConsTracking(int(options.max_num_objects), options.mnd, options.division_threshold, rf_fn, bool(options.size_dependent_detection_prob), options.forbidden_cost, options.ep_gap, obj_size[0], not bool(options.without_tracklets), options.division_weight, options.transition_weight, not bool(options.without_divisions), options.disappearance_cost, options.appearance_cost, options.with_merger_resolution, ndim, options.trans_par, options.border_width, fov, not bool(options.woconstr))
        tracker = track.ConsTracking(int(options.max_num_objects), bool(options.size_dependent_detection_prob), obj_size[0], options.mnd,
                                     not bool(options.without_divisions), options.division_threshold, rf_fn, fov, "none")
    elif (options.method == "chaingraph"):
        rf_fn = 'none'
        with_rf = False
        if options.rf_fn:
            rf_fn = options.rf_fn
            with_rf = True
        tracker = track.ChaingraphTracking(rf_fn, options.appearance_cost, options.disappearance_cost, options.det, options.mis, with_rf,
                                           options.opp, options.forbidden_cost, not (options.without_constraints),
                                           options.fixed_detections, options.mdd, options.ma, options.ep_gap,
                                           options.nnc)
    else:
        raise Exception("unknown tracking method: " + options.method)

    if options.method == 'conservation':

        sigma_vec = track.VectorOfDouble()
        for si in options.perturb_sigmas:
            sigma_vec.append(si)

        if options.perturbation_distribution == "DiverseMbest":
            distrib_type = track.DistrId.DiverseMbest
        elif options.perturbation_distribution == "MbestCPLEX":
            distrib_type = track.DistrId.MbestCPLEX
        elif options.perturbation_distribution == "ClassifierUncertainty":
            distrib_type = track.DistrId.ClassifierUncertainty
        elif options.perturbation_distribution == "GaussianPerturbation":
            distrib_type = track.DistrId.GaussianPerturbation
        elif options.perturbation_distribution == "PerturbAndMAP":
            distrib_type = track.DistrId.PerturbAndMAP
    
        if(options.eff and len(options.gt_pth) > 0): # generate structured learning files

            g = store_label_in_hypotheses_graph()

            #label full graph
            n_it = track.NodeIt(g)
            a_it = track.ArcIt(g)
            tmap = g.getNodeTraxelMap()
                
            nodeTimestepIdMap = {}
            arcTimestepIdMap = {}

            for n in n_it:
                #print tmap[n].Timestep,tmap[n].Id
                nodeTimestepIdMap[tmap[n].Timestep,tmap[n].Id] =  n
                g.addAppearanceLabel(n,0) 
                g.addDisappearanceLabel(n,0) 
                g.addDivisionLabel(n,0) 

            for a in a_it:
                #print tmap[g.source(a)].Timestep,tmap[g.source(a)].Id,tmap[g.target(a)].Id
                g.addArcLabel(a,0)
                arcTimestepIdMap[tmap[g.source(a)].Timestep,tmap[g.source(a)].Id,tmap[g.target(a)].Id] = a


               #load ground truth from hd5 files and label graph accordingly
            for i in range(g.earliest_timestep(), g.latest_timestep()+1):
                print "open file:",options.gt_pth.rstrip('/')+'/'+"{0:04d}".format(i)+'.h5'
                f = h5py.File(options.gt_pth.rstrip('/')+'/'+"{0:04d}".format(i)+'.h5','r')
                print f


                try:
                    applist = f["tracking"]["Appearances"]
                except:
                    applist = []

                try:
                    dislist = f["tracking"]["Disappearances"]
                except:
                    dislist = []
                
                try:
                    movlist = f["tracking"]["Moves"]
                    print movlist
                except:
                    movlist = []
                  
                try:
                    spllist = f["tracking"]["Splits"]
                except:
                    spllist = []

                try:
                    merlist = f["tracking"]["Mergers"]
                except:
                    merlist = []

                mdic = {}
                for m in merlist:
                    mdic[m[0]] = np.asscalar(m[1])
                  
                # if(g.earliest_timestep() == i):
                #     try:
                #         idlist = f["objects/meta/id"]
                #     except:
                #         idlist = []

                #     for earlyIds in idlist:
                #         if((i,earlyIds) in nodeTimestepIdMap):
                #             g.addAppearanceLabel(nodeTimestepIdMap[i,earlyIds],1)
                #             g.addDisappearanceLabel(nodeTimestepIdMap[i,earlyIds],1)
                #         else:
                #             print "ERROR IN app",i,earlyIds                


                for appset in applist:
                    if((i,appset[0]) in nodeTimestepIdMap):
                        g.addAppearanceLabel(nodeTimestepIdMap[i,appset[0]],mdic.get(appset[0],1))
                        g.addDisappearanceLabel(nodeTimestepIdMap[i,appset[0]],0)
                    else:
                        print "ERROR IN app",i,appset[0]
                for disset in dislist:
                    if((i-1,disset[0]) in nodeTimestepIdMap):
                        g.addAppearanceLabel(nodeTimestepIdMap[i-1,disset[0]],0)
                        g.addDisappearanceLabel(nodeTimestepIdMap[i-1,disset[0]],mdic.get(disset[0],1))
                    else:
                        print "ERROR IN disapp",i-1,disset[0]
                for movset in movlist:
                    if((i-1,movset[0]) in nodeTimestepIdMap and (i,movset[1])in nodeTimestepIdMap):
                        g.addAppearanceLabel(nodeTimestepIdMap[i-1,movset[0]],mdic.get(movset[0],1))
                        g.addDisappearanceLabel(nodeTimestepIdMap[i,movset[1]],mdic.get(movset[1],1))
                        if((i-1,movset[0],movset[1]) in arcTimestepIdMap):
                            g.addArcLabel(arcTimestepIdMap[i-1,movset[0],movset[1]],1)
                        else:
                            print "ERROR3 IN move",i-1,movset[0],movset[1],"  adding new arc"
                            newarc = g.addArc(nodeTimestepIdMap[i-1,movset[0]],nodeTimestepIdMap[i,movset[0]])
                            g.addArcLabel(newarc,1)

                for splset in spllist:
                    if((i-1,splset[0],splset[1]) in arcTimestepIdMap and (i-1,splset[0],splset[2]) in arcTimestepIdMap):
                        g.addArcLabel(arcTimestepIdMap[i-1,splset[0],splset[1]],1)
                        g.addArcLabel(arcTimestepIdMap[i-1,splset[0],splset[2]],1)
                    else:
                        print "ERROR IN SPLIT arc", i-1,splset[0],splset[2]
                    if((i-1,splset[0]) in nodeTimestepIdMap and (i,splset[1]) in nodeTimestepIdMap and (i,splset[2]) in nodeTimestepIdMap ):
                        g.addDivisionLabel(nodeTimestepIdMap[i-1,splset[0]],1)
                        g.addAppearanceLabel(nodeTimestepIdMap[i,splset[1]],mdic.get(splset[1],1))
                        g.addAppearanceLabel(nodeTimestepIdMap[i,splset[2]],mdic.get(splset[2],1))
                        g.addDisappearanceLabel(nodeTimestepIdMap[i-1,splset[0]],1)
                    else:
                        print "ERROR IN SPLIT node",i-1,splset[0],splset[2]




            # g.set_injected_solution()
            # events = track.getEventsOfGraph(g)[1:]

            # print "Saving events..."
            # print "Length of events " + str(len(events))

            # working_fns = [ "gt_new/%04d.h5" % timestep for timestep in xrange(t0,t1+1)]
            # print "len(working_fns)=" + str(len(working_fns)) 
            # assert(len(events) + 1 == len(working_fns))
            # with h5py.File(fns[0], 'r') as src_file:
            #     # first timestep without tracking
            #     with io.LineageH5(working_fns[0], 'w') as dest_file:
            #         print "-- writing empty tracking to base file", working_fns[0]
            #         meta = dest_file.create_group('objects/meta')
            #         m = max_traxel_id_at[0]
            #         ids = np.asarray(range(m+1))
            #         ids = ids[ids > 0]
            #         valid = np.ones(ids.shape)
            #         meta.create_dataset("id", data=ids, dtype=np.uint32)
            #         meta.create_dataset("valid", data=valid, dtype=np.uint32)

            #         # create empty tracking group
            #         dest_file.create_group('tracking')
            #         print "-> base file successfully written"

                    
            #     # tracked timesteps
            #     for i, events_at in enumerate(events):
            #         with io.LineageH5(working_fns[i+1], 'w') as dest_file:
            #             # [t0+1+i,...] or [t0+i,...]?
            #             #shape = src_file[trans_vector_path].shape
            #             #li_name = label_im_path % (t0+i+1, t0+i+2, shape[1], shape[2], shape[3])
            #             #label_img = np.array(src_file[li_name][0,...,0])

            #             meta = dest_file.create_group('objects/meta')
            #             m = max_traxel_id_at[i+1]
            #             ids = np.asarray(range(m+1))
            #             #ids = np.unique(label_img)
            #             ids = ids[ids > 0]
            #             valid = np.ones(ids.shape)
            #             meta.create_dataset("id", data=ids[::-1], dtype=np.uint32)
            #             meta.create_dataset("valid", data=valid, dtype=np.uint32)

            #         write_events(events_at, working_fns[i + 1])


            # exit()

            tracker.SetFunkeyExportLabeledGraph(True)

            weights = track.WeightVector()

            outpath = options.out_dir.rstrip('/') +"/"+"m_"+str(int(options.max_num_objects))+"_start"+str(g.earliest_timestep())+"_end"+str(g.latest_timestep())+"_woc"+str(options.woc)+"lw"+options.funkey_weights.replace(" ", "_")+"reg"+str(options.funkey_regularizerWeight)
            os.system("mkdir -p "+outpath)

            #remove learning files if present
            os.system("rm "+outpath+"/features_0.txt")
            os.system("rm "+outpath+"/features.txt")
            os.system("rm "+outpath+"/constraints.txt")
            os.system("rm "+outpath+"/labels.txt")

            uncertaintyParam = track.UncertaintyParameter(1,
                                                distrib_type,
                                                sigma_vec)


            tracker.writeFunkeyFiles(ts,outpath+"/features.txt",
                                        outpath+"/constraints.txt",
                                        outpath+"/labels.txt",
                                        weights,
                                        uncertaintyParam,
                                        options.forbidden_cost,
                                        ndim,
                                        not bool(options.without_tracklets),
                                        options.trans_par,
                                        options.border_width,
                                        trans_classifier)

            os.system("rm "+outpath+"/features.txt")


            if(options.learn_funkey):

                print "calling funkey"

                learnedWeights = tracker.LearnWithFunkey(outpath+"/features_0.txt",
                                                         outpath+"/constraints.txt",
                                                         outpath+"/labels.txt",
                                                         options.funkey_weights,
                                                         "--regularizerWeight="+str(options.funkey_regularizerWeight))

                learnedWeightsName = ["  --det=","  --div=","  --tr=","  --dis=","  --app="]

                f = open(outpath+'/weights.txt','w')
                for i,w in enumerate(learnedWeights):
                    f.write(learnedWeightsName[i] + str(w))
                f.close()

                detw = learnedWeights[0]
                divw = learnedWeights[1]
                traw = learnedWeights[2]
                disw = learnedWeights[3]
                appw = learnedWeights[4]

                tracker.SetFunkeyExportLabeledGraph(False)
                tracker.SetFunkeyOutputFiles("","",outpath+"/tracking_labels.txt",True,uncertaintyParam)

                events_all_perturbations = tracker.track(
                    options.forbidden_cost,
                    options.ep_gap,
                    not bool(options.without_tracklets),
                    detw,
                    divw,
                    traw,
                    disw,
                    appw,
                    options.with_merger_resolution,
                    ndim,
                    options.trans_par,
                    options.border_width,
                    not bool(options.woconstr),
                    uncertaintyParam,
                    options.timeout,
                    trans_classifier
                )


                events = events_all_perturbations[0][1:]

                print "Saving events..."
                print "Length of events " + str(len(events))

                working_fns = [outpath+ "/%04d.h5" % timestep for timestep in xrange(t0,t1+1)]
                print "len(working_fns)=" + str(len(working_fns))
                assert(len(events) + 1 == len(working_fns))
                with h5py.File(fns[0], 'r') as src_file:
                    # first timestep without tracking
                    with io.LineageH5(working_fns[0], 'w') as dest_file:
                        print "-- writing empty tracking to base file", working_fns[0]
                        meta = dest_file.create_group('objects/meta')
                        m = max_traxel_id_at[0]
                        ids = np.asarray(range(m+1))
                        ids = ids[ids > 0]
                        valid = np.ones(ids.shape)
                        meta.create_dataset("id", data=ids, dtype=np.uint32)
                        meta.create_dataset("valid", data=valid, dtype=np.uint32)

                        # create empty tracking group
                        dest_file.create_group('tracking')
                        print "-> base file successfully written"


                    # tracked timesteps
                    for i, events_at in enumerate(events):
                        with io.LineageH5(working_fns[i+1], 'w') as dest_file:
                            # [t0+1+i,...] or [t0+i,...]?
                            #shape = src_file[trans_vector_path].shape
                            #li_name = label_im_path % (t0+i+1, t0+i+2, shape[1], shape[2], shape[3])
                            #label_img = np.array(src_file[li_name][0,...,0])

                            meta = dest_file.create_group('objects/meta')
                            m = max_traxel_id_at[i+1]
                            ids = np.asarray(range(m+1))
                            #ids = np.unique(label_img)
                            ids = ids[ids > 0]
                            valid = np.ones(ids.shape)
                            meta.create_dataset("id", data=ids[::-1], dtype=np.uint32)
                            meta.create_dataset("valid", data=valid, dtype=np.uint32)

                        write_events(events_at, working_fns[i + 1])

                #call compare script

                #copy ground truth files
                os.system("mkdir -p "+outpath+"/groundtruth")
                for timestep in xrange(t0,t1+1):
                    os.system("cp "+options.gt_pth \
                              +"{0:04d}".format(timestep)+'.h5'+" "+outpath+"/groundtruth/" \
                              +"{0:04d}".format(timestep)+'.h5')


                # print options.compare_path +" " \
                # +outpath+"/groundtruth/  "+ outpath +" > "+outpath+"/compare.txt"

                print options.compare_path + "/h5files_to_onefile "+outpath+"/combined_tracking.h5 ./training_object-classification.ilp "+outpath+"/00*.h5"
                os.system(options.compare_path + "/h5files_to_onefile "+outpath+"/combined_tracking.h5 ./training_object-classification.ilp "+outpath+"/00*.h5")


                print options.compare_path + "/joint_tracking_evaluation --ground-truth ./training_tracking_gt.h5 --tracking-result "+outpath+"/combined_gt.h5 --threshold 0.5 --tracking-ground-truth-group tracking --tracking-result-group eventVector --ground-truth-internal-path label_image_T --tracking-result-internal-path LabelImage --use-hdf5"
                os.system(options.compare_path + "/joint_tracking_evaluation --ground-truth ./training_tracking_gt.h5 --tracking-result "+outpath+"/combined_gt$i.h5 --threshold 0.5 --tracking-ground-truth-group tracking --tracking-result-group eventVector --ground-truth-internal-path label_image_T --tracking-result-internal-path LabelImage --use-hdf5")
                

                print options.compare_path +"compare_tracking "\
                    +outpath+"/groundtruth/  "+ outpath +" > "+outpath+"/compare.txt"
                os.system(options.compare_path +"compare_tracking "\
                    +outpath+"/groundtruth/  "+ outpath +" > "+outpath+"/compare.txt")


            if(options.learn_perturbation_weights):

                print "calling learn_perturbation_weights"
                tracker.SetFunkeyExportLabeledGraph(False)

                optimal_loss = 1e+75;
                optimal_parameters = []

                os.system("mkdir -p "+outpath+"/pertubation_labels")
                f = open(outpath+"/pertubation_labels/pertubation.log",'w')


                weights = track.VectorOfDouble()

                for pertubation_parameter in itertools.product(np.arange(-1, 2, 1), repeat=3):

                    parameter_name = ""

                    sigma_vec = track.VectorOfDouble()

                    sigma_vec.append(float(0)) # offset for appearance (not used)
                    sigma_vec.append(float(0)) # offset for disappearance (not used)

                    for si in pertubation_parameter:
                        sigma_vec.append(float(10**si))
                        parameter_name += "_"+ str(float(10**si))

                    uncertaintyParam = track.UncertaintyParameter(options.num_iterations,
                                                                  distrib_type,
                                                                  sigma_vec)

                    # save labels of proposals to file:
                    tracker.SetFunkeyOutputFiles('', '', outpath + '/pertubation_labels/perturbed_labeling'+parameter_name+'.txt', False,uncertaintyParam)

                    all_events = tracker.track(
                        options.forbidden_cost,
                        options.ep_gap,
                        not bool(options.without_tracklets),
                        options.detection_weight,
                        options.division_weight,
                        options.transition_weight,
                        options.disappearance_cost,
                        options.appearance_cost,
                        options.with_merger_resolution,
                        ndim,
                        options.trans_par,
                        options.border_width,
                        not bool(options.woconstr),
                        uncertaintyParam,
                        options.timeout,
                        trans_classifier, # pointer to transition classifier
                    )


                    loss = tracker.HamminglossOfFiles(outpath+"/labels.txt",outpath+'/pertubation_labels/perturbed_labeling'+parameter_name+'_'+str(options.num_iterations-1)+'.txt')

                    f.write(str(loss) +"\t"+ str(pertubation_parameter)+ "\n")

                    if(loss < optimal_loss):
                        optimal_loss = loss
                        optimal_parameters = pertubation_parameter
                        print "found better pertubation parameters:  loss:" ,optimal_loss,"  para: ",optimal_parameters
                f.close()
                print "result:   found solution with loss:", optimal_loss ," with parameters ", optimal_parameters
            exit()

        else:
            train_outlier_svm(options)

            """
            C++ signature of operator():

            ConsTracking& tr, 
            TraxelStore& ts, 
            TimestepIdCoordinateMapPtr& coordinates,
            double forbidden_cost,
            double ep_gap,
            bool   with_tracklets,
            double division_weight,
            double transition_weight,
            double disappearance_cost,
            double appearance_cost,
            bool   with_merger_resolution,
            int    n_dim,
            double transition_parameter,
            double border_width,
            UncertaintyParameter uncertaintyParam,
            bool   with_constraints,
            double cplex_timeout
            """
            

            uncertaintyParam = track.UncertaintyParameter(options.num_iterations,
                                                          distrib_type,
                                                          sigma_vec)

            feature_vector_filename = options.out_dir.rstrip('/') + '/ho_feature_vectors.txt'
            try:
                os.remove(feature_vector_filename)
            except:
                pass

            hypotheses_graph = tracker.buildGraph(ts)

            # save labels of proposals when doing inference
            # tracker.SetFunkeyOutputFiles('',
            #                              '',
            #                              options.out_dir.rstrip('/') + '/proposal_labeling',
            #                              False,
            #                              uncertaintyParam)

            # perform the real tracking
            all_events = tracker.track(
                options.forbidden_cost,
                options.ep_gap,
                not bool(options.without_tracklets),
                options.detection_weight,
                options.division_weight,
                options.transition_weight,
                options.disappearance_cost,
                options.appearance_cost,
                options.with_merger_resolution,
                ndim,
                options.trans_par,
                options.border_width,
                not bool(options.woconstr),
                uncertaintyParam,
                options.timeout,
                trans_classifier, # pointer to transition classifier
            )

            # # write out feature matrix for each proposal, but use original traxel store
            # # will later be multiplied by label to get featurevector
            # tracker.SetFunkeyOutputFiles(options.out_dir.rstrip('/') + '/feature_matrix',
            #                              '',
            #                              '',
            #                              False,
            #                              uncertaintyParam)
            # tracker.writeAllFunkeyFeatures(ts,
            #                                uncertaintyParam,
            #                                options.forbidden_cost,
            #                                ndim,
            #                                not bool(options.without_tracklets),
            #                                options.trans_par,
            #                                options.border_width
            # )


            # run merger resolving
            if options.with_merger_resolution and options.max_num_objects > 1 and len(options.raw_filename) > 0:
                with h5py.File(fns[0], 'r') as h5file:  # open file to access label images
                    feature_vectors = []
                    for i, event_vector in enumerate(all_events):  # go through all solutions
                        try:
                            os.makedirs(options.out_dir.rstrip('/') + '/iter_' + str(i))
                        except:
                            pass

                        print("Resolving mergers for solution {}".format(i))
                        coordinate_map = track.TimestepIdCoordinateMap()
                        coordinate_map.initialize()
                        num_mergers = 0
                        for timestep, timestep_events in enumerate(event_vector):
                            for event in timestep_events:
                                if event.type == track.EventType.Merger:
                                    traxel = ts.get_traxel(event.traxel_ids[0], timestep)
                                    try:
                                        extract_coordinates(coordinate_map, h5file, traxel, options)
                                    except Exception as e:
                                        print("Error when extracting coordinates for id={} in timestep={}".format(event.traxel_ids[0], timestep))
                                        print(type(e))
                                        print(e)
                                        raise Exception
                                    num_mergers += 1

                        print("Found {} merger events in proposal {}. Resolving them...".format(num_mergers, i))

                        hypotheses_graph.set_solution(i)
                        resolved_events = tracker.resolve_mergers(
                            event_vector,
                            coordinate_map.get(),
                            float(options.ep_gap),
                            options.transition_weight,
                            not bool(options.without_tracklets),
                            ndim,
                            options.trans_par,
                            not bool(options.woconstr))

                        with h5py.File(options.raw_filename, 'r') as raw_h5:
                            print("Update labelimage and compute new features after merger resolving "
                                  "for solution {}".format(i))
                            for timestep, timestep_events in enumerate(resolved_events):
                                for event in timestep_events:
                                    if event.type == track.EventType.ResolvedTo:
                                        merger_traxel = ts.get_traxel(event.traxel_ids[0], timestep)
                                        new_traxel_ids = event.traxel_ids[1:]
                                        update_merger_features(coordinate_map,
                                                               h5file,
                                                               merger_traxel,
                                                               new_traxel_ids,
                                                               raw_h5,
                                                               options,
                                                               fs)
                        resolved_graph = tracker.get_resolved_hypotheses_graph()
                        # extract features of this solution
                        feature_vector, score = extract_features_and_compute_score(options, i, resolved_graph, ts, fov, feature_vector_filename)
                        feature_vectors.append(feature_vector)

                    print("Storing feature vectors of all solutions")
                    np.savetxt(options.out_dir.rstrip('/') + '/feature_vectors.txt', np.array(feature_vectors).transpose())
            events = all_events[0]
            first_events = events[0]
            events = events[1:]
    else:
        events = tracker(ts)

    processing_pool = Pool()

    if not options.skip_saving:
        for i in range(len(all_events)):
            events = all_events[i]
            first_events = events[0]
            events = events[1:]

            processing_pool.apply_async(save_events, (options.out_dir.rstrip('/') + '/iter_' + str(i), copy.copy(events), t0, t1,
                                                      options.label_img_path, max_traxel_id_at, fns[0]))

    processing_pool.close()
    processing_pool.join()

    stop = time.time()
    since = stop - start
    print "Elapsed time [s]: " + str(int(since))
    print "Elapsed time [min]: " + str(int(since) / 60)
    print "Elapsed time [h]: " + str(int(since) / 3600)
