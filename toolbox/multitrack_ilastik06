#!/usr/bin/env python
import sys
sys.path.append('../.')
sys.path.append('.')

import os
import os.path as path
import getpass
import glob
import optparse
import socket
import shutil
import time
import numpy as np
import h5py
import vigra
#from empryonic import track
import pgmlink as track
from empryonic import io




def get_feature(h5file, feature_path, object_id):
    if not feature_path in h5file:
        raise Exception("Feature %s not found in %s." % (feature_path, h5file.filename))
    return h5file[feature_path][object_id,...]
    


def generate_traxelstore(h5file,
             options,
			 feature_path,
			 time_range,
			 x_range,
			 y_range,
			 z_range,
			 size_range,
			 x_scale=1.0,
			 y_scale=1.0,
			 z_scale=1.0,
			 with_div=True,
			 with_local_centers=False,
			 median_object_size=None,
			 max_traxel_id_at=None,
          with_merger_prior=True,
          with_coordinates=False,
          max_num_mergers=1,
          with_optical_correction=False,
          ext_probs=None,          
          ):

    
    print "generating traxels"
    print "fetching region features and division probabilities"
    print h5file.filename, feature_path

    #division_rf = None
    #h5filename = str(h5file.filename)
    #if with_div:
    #    rf_path = 'DivisionDetection/ClassifierForests/Forest0000'
    #    if  rf_path in h5file:
    #        h5file.close()
    #        division_rf = vigra.learning.RandomForest(h5filename, rf_path)
    #        h5file = h5py.File(h5filename, 'r')

    #merger_rf = None
    #if with_merger_prior:
    #    print "with_merger_prior"
    #    rf_path = 'CellClassification/ClassifierForests/Forest0000'
    #    if not rf_path in h5file:
    #        raise Exception("%s does not contain path %s" % (h5file.filename, rf_path))
    #    h5file.close()
    #    merger_rf = vigra.learning.RandomForest(h5filename, rf_path)
    #    h5file = h5py.File(h5filename, 'r')

    #feats = h5file[feature_path]
    #feats = h5file[feature_path]['samples/0']

    
    if with_div: # and 'ClassProbabilities' in h5file[feature_path].keys():
        #divProbs = None # self.ClassMapping(time_range).wait()
        #divProbs = h5file[feature_path]['ClassProbabilities']
        divProbs = h5file[options.div_prob_path]
	
    if with_merger_prior:
        detProbs = h5file[options.obj_count_path]

    if with_local_centers:
        localCenters = None # self.RegionLocalCenters(time_range).wait()
        
    print "filling traxelstore"
    ts = track.TraxelStore()

    if x_range is None:
        x_range = [0, sys.maxint]

    if y_range is None:
        y_range = [0, sys.maxint]

    if z_range is None:
        z_range = [0, sys.maxint]

    shape_t = len(h5file[options.obj_count_path].keys())
    #shape_t = #h5file[options.trans_vector_path].shape[0]
    keys_sorted = range(shape_t)
    #keys_sorted = sorted(feats.keys(), key=int)
    if time_range is not None:
        keys_sorted = [key for key in keys_sorted if int(key) >= time_range[0] and int(key) < time_range[1]]
    
    
    max_traxel_id_at = track.VectorOfInt()  
    filtered_labels = {}        
    obj_sizes = []
    total_count = 0
    empty_frame = False

    for t in keys_sorted:
        feats_name = options.feats_path % (t, t+1, 'RegionCenter')
        #rc = np.array(feats[t]['0']['RegionCenter'])
        rc = np.array(h5file[feats_name])
        if rc.size:
           rc = rc[1:, ...]
        if with_optical_correction:
           try:
              feats_name = options.feats_path % (t, t+1, 'RegionCenter_corr')
              rc_corr = np.array(h5file[feats_name])
           except:
              raise Exception, 'cannot consider optical correction since it has not been computed'
           if rc_corr.size:
              rc_corr = rc_corr[1:,...]

        feats_name = options.feats_path % (t, t+1, 'Count')
        #ct = np.array(feats[t]['0']['Count'])
        ct = np.array(h5file[feats_name])
        if ct.size:
           ct = ct[1:, ...]

        if with_coordinates:
           feats_name = options.feats_path % (t, t+1, 'Coord<ValueList>')
           #coordinates_dataset = feats[t]['0']['Coord<ValueList >']
           #stride = coordinates_dataset.attrs['stride']
           #n_obj = coordinates_dataset.attrs['n_objects']
           #coordinates = [[0]]
           coordinates_grp = h5file[feats_name]
           coordinates = []
           for obj_id in sorted([int(x) for x in coordinates_grp.keys()]):
               values = coordinates_grp[str(obj_id)]
               coordinates.append(np.array(values.value, dtype=object))
           if len(coordinates):
               coordinates = coordinates[1:]
           #for idx in xrange(stride, stride*n_obj, stride):
               #max_valid = np.where(coordinates_dataset[idx:idx+stride,0] == -1)
               #curr_stride = stride
               #if max_valid[0].shape[0] > 0:
               #    curr_stride = max_valid[0][0]
               #coordinates.append(np.array(coordinates_dataset[idx:idx+curr_stride,...]))
           #if len(coordinates):
           #    coordinates = coordinates[1:]
            
            
        print "at timestep ", t, rc.shape[0], "traxels found"
        count = 0
        filtered_labels[t] = []
        for idx in range(rc.shape[0]):
            if len(rc[idx]) == 2:
               x, y = rc[idx]
               z = 0
            elif len(rc[idx]) == 3:
               x, y, z = rc[idx]
            else:
               raise Exception, "The RegionCenter feature must have dimensionality 2 or 3."
            size = ct[idx]
            if (x < x_range[0] or x >= x_range[1] or
                y < y_range[0] or y >= y_range[1] or
                z < z_range[0] or z >= z_range[1] or
                size < size_range[0] or size >= size_range[1]):
                filtered_labels[t].append(int(idx + 1))
                continue
            else:
                count += 1
            tr = track.Traxel()
            tr.set_x_scale(x_scale)
            tr.set_y_scale(y_scale)
            tr.set_z_scale(z_scale)
            tr.Id = int(idx + 1)
            tr.Timestep = int(t)
            
            tr.add_feature_array("com", 3)                
            for i, v in enumerate([x,y,z]):
                tr.set_feature_value('com', i, float(v))

            if with_coordinates:
               tr.add_feature_array("coordinates", 3*len(coordinates[idx]))
               for i, v in enumerate(coordinates[idx]):
                   tr.set_feature_value("coordinates", 3*i,   float(v[0]))
                   tr.set_feature_value("coordinates", 3*i+1, float(v[1]))
                   if len(v) == 2:
                     tr.set_feature_value("coordinates", 3*i+2, 0.)
                   elif len(v) == 3:
                     tr.set_feature_value("coordinates", 3*i+2, float(v[2]))
                   else:
                     raise Exception, "dimensions must be 2 or 3"
            
            if with_optical_correction:
                tr.add_feature_array("com_corrected", 3)
                for i, v in enumerate(rc_corr[idx]):
                    tr.set_feature_value("com_corrected", i, float(v))
                if len(rc_corr[idx]) == 2:
                    tr.set_feature_value("com_corrected", 2, 0.)
 
            if with_div:
                tr.add_feature_array("divProb", 1)
                prob = 0.0

                #if  division_rf:
                #    rf = division_rf # vigra.learning.RandomForest(str(h5file.filename), rf_path)
                #    features = np.array([], dtype=np.float32)
                #    for div_feature in selected_features_division_detection:
                #        features = np.append(features, get_feature(h5file, ilp_feature_path % (t, div_feature), tr.Id).astype(np.float32))
                #    prob = float(rf.predictProbabilities(features[np.newaxis,...])[0][1])
                #    #print selected_features_division_detection
                #    #print features
                #else:
                prob = float(divProbs[str(t)][idx+1][1])
                # idx+1 because rc and ct start from 1, divProbs starts from 0
                tr.set_feature_value("divProb", 0, prob)
            
            if with_local_centers:
                raise Exception, "not yet implemented"
                tr.add_feature_array("localCentersX", len(localCenters[t][idx+1]))  
                tr.add_feature_array("localCentersY", len(localCenters[t][idx+1]))
                tr.add_feature_array("localCentersZ", len(localCenters[t][idx+1]))            
                for i, v in enumerate(localCenters[t][idx+1]):
                    tr.set_feature_value("localCentersX", i, float(v[0]))
                    tr.set_feature_value("localCentersY", i, float(v[1]))
                    tr.set_feature_value("localCentersZ", i, float(v[2]))


            if with_merger_prior and ext_probs is None:
                tr.add_feature_array("detProb", max_num_mergers+1)
                probs = []
                for i in range(len(detProbs[str(t)][idx+1])):
                    probs.append(float(detProbs[str(t)][idx+1][i]))
                probs[max_num_mergers] = sum(probs[max_num_mergers:])
                for i in range(max_num_mergers+1):
                    tr.set_feature_value("detProb", i, float(probs[i]))

            elif with_merger_prior and ext_probs is not None:
                assert max_num_mergers == 1, "not implemented for max_num_mergers > 1"
                detProbFilename = ext_probs % t
                detProbGroup = h5py.File(detProbFilename, 'r')['objects/meta']
                traxel_index = np.where(detProbGroup['id'].value == tr.Id)[0][0]
                detProbFeat = [detProbGroup['prediction_class0'].value[traxel_index],
                              detProbGroup['prediction_class1'].value[traxel_index]]
                tr.add_feature_array("detProb", 2)
                for i in xrange(len(detProbFeat)):
                   tr.set_feature_value("detProb", i, float(detProbFeat[i]))
                
                #if merger_rf:
                #    rf = merger_rf # vigra.learning.RandomForest(str(h5file.filename), rf_path)
                #    tr.add_feature_array("detProb", rf.labelCount())
                #    features = np.array([], dtype=np.float32)
                #    for div_feature in selected_features_cell_classification:
                #        features = np.append(features, get_feature(h5file, ilp_feature_path % (t, div_feature), tr.Id).astype(np.float32))
                #    # print t, idx
                #    # print selected_features_cell_classification
                #    # print features
                #    detProbFeat = rf.predictProbabilities(features[np.newaxis,...])[0]
                #    for idx in xrange(len(detProbFeat)):
                #        tr.set_feature_value("detProb", idx, float(detProbFeat[idx]))
            
            tr.add_feature_array("count", 1)
            tr.set_feature_value("count", 0, float(size))
            if median_object_size is not None:
                obj_sizes.append(float(size))
            ts.add(tr)   
                     
        print "at timestep ", t, count, "traxels passed filter"
        max_traxel_id_at.append(int(rc.shape[0]))
        if count == 0:
            empty_frame = True
            
        total_count += count
    
    if median_object_size is not None:
        median_object_size[0] = np.median(np.array(obj_sizes),overwrite_input=True)
        print 'median object size = ' + str(median_object_size[0])
    
    return ts, max_traxel_id_at # , filtered_labels, empty_frame


def write_detections(detections, fn):
	with io.LineageH5(fn, 'r') as f:
		traxel_ids = f['objects/meta/id'].value
		valid = f['objects/meta/valid'].value
	assert(len(traxel_ids) == len(valid))
	assert(len(detections) == len(np.flatnonzero(valid)))

	detection_indicator = np.zeros(len(traxel_ids), dtype=np.uint16)

	for i, traxel_id in enumerate(traxel_ids):
		if valid[i] != 0:
			if detections[int(traxel_id)]:
				detection_indicator[i] = 1
			else:
				detection_indicator[i] = 0
	
	with io.LineageH5(fn, 'r+') as f:
            # delete old dataset
            if "detection" in f['objects/meta'].keys():
                del f["objects/meta/detection"]
	    f.create_dataset('objects/meta/detection', data=detection_indicator)
            

def write_events(events, fn):
        dis = []
        app = []
        div = []
        mov = []
        mer = []
        mul = []
        print "-- Writing results to " + path.basename(fn)
        for event in events:
            if event.type == track.EventType.Appearance:
                app.append((event.traxel_ids[0], event.energy))
            if event.type == track.EventType.Disappearance:
                dis.append((event.traxel_ids[0], event.energy))
            if event.type == track.EventType.Division:
                div.append((event.traxel_ids[0], event.traxel_ids[1], event.traxel_ids[2], event.energy))
            if event.type == track.EventType.Move:
                mov.append((event.traxel_ids[0], event.traxel_ids[1], event.energy))
            if event.type == track.EventType.Merger:
                mer.append((event.traxel_ids[0], event.traxel_ids[1], event.energy))
            if event.type == track.EventType.MultiFrameMove:
                mul.append(tuple(event.traxel_ids) + (event.energy,))

        # convert to ndarray for better indexing
        dis = np.asarray(dis)
        app = np.asarray(app)
        div = np.asarray(div)
        mov = np.asarray(mov)
        mer = np.asarray(mer)
        mul = np.asarray(mul)

        # write only if file exists
        with io.LineageH5(fn, 'r+') as f_curr:
            # delete old tracking
            if "tracking" in f_curr.keys():
                del f_curr["tracking"]

            tg = f_curr.create_group("tracking")
            
            # write associations
            if len(app):
                ds = tg.create_dataset("Appearances", data=app[:, :-1], dtype=np.int32)
                ds.attrs["Format"] = "cell label appeared in current file"

                ds = tg.create_dataset("Appearances-Energy", data=app[:, -1], dtype=np.double)
                ds.attrs["Format"] = "lower energy -> higher confidence"

            if len(dis):
                ds = tg.create_dataset("Disappearances", data=dis[:, :-1], dtype=np.int32)
                ds.attrs["Format"] = "cell label disappeared in current file"

                ds = tg.create_dataset("Disappearances-Energy", data=dis[:, -1], dtype=np.double)
                ds.attrs["Format"] = "lower energy -> higher confidence"


            if len(mov):
                ds = tg.create_dataset("Moves", data=mov[:, :-1], dtype=np.int32)
                ds.attrs["Format"] = "from (previous file), to (current file)"

                ds = tg.create_dataset("Moves-Energy", data=mov[:, -1], dtype=np.double)
                ds.attrs["Format"] = "lower energy -> higher confidence"

                
            if len(div):
                ds = tg.create_dataset("Splits", data=div[:, :-1], dtype=np.int32)
                ds.attrs["Format"] = "ancestor (previous file), descendant (current file), descendant (current file)"

                ds = tg.create_dataset("Splits-Energy", data=div[:, -1], dtype=np.double)
                ds.attrs["Format"] = "lower energy -> higher confidence"

	    if len(mer):
                ds = tg.create_dataset("Mergers", data=mer[:, :-1], dtype=np.int32)
                ds.attrs["Format"] = "descendant (current file), number of objects"
		
                ds = tg.create_dataset("Mergers-Energy", data=mer[:, -1], dtype=np.double)
                ds.attrs["Format"] = "lower energy -> higher confidence"

            if len(mul):
                ds = tg.create_dataset("MultiFrameMoves", data=mul[:, :-1], dtype=np.int32)
                ds.attrs["Format"] = "from (given by timestep), to (current file), timestep"

                ds = tg.create_dataset("MultiFrameMoves-Energy", data=mul[:, -1], dtype=np.double)
                ds.attrs["Format"] = "lower energy -> higher confidence"
                

        print "-> results successfully written"


        
if __name__ == "__main__":
    usage = """%prog [options] FILES
Track cells.

Before processing, input files are copied to OUT_DIR. Groups, that will not be modified are not
copied but linked to the original files to improve execution speed and storage requirements.
"""

    parser = optparse.OptionParser(usage=usage)
    parser.add_option('--method', type='str', default='conservation', help='chaingraph or conservation [default: %default]')
    parser.add_option('-o', '--output-dir', type='str', dest='out_dir', default='tracked', help='[default: %default]')
    parser.add_option('--x-scale', type='float', dest='x_scale', default=1., help='[default: %default]')
    parser.add_option('--y-scale', type='float', dest='y_scale', default=1., help='[default: %default]')
    parser.add_option('--z-scale', type='float', dest='z_scale', default=1., help='[default: %default]')
    parser.add_option('--with-visbricks', action='store_true', dest='with_visbricks', help='write out a "time dependent HDF5" file for visbricks')
    parser.add_option('--with-rel-linking', action='store_true', dest='with_rel_linking', help='link hdf5 files relative instead of absolute')
    parser.add_option('--full-copy', action='store_true', dest='full_copy', help='do not link to but copy input files completely')
    parser.add_option('--user', type='str', dest='user', default=getpass.getuser(), help='user to log [default: %default]')
    parser.add_option('--date', type='str', dest='date', default=time.ctime(), help='datetime to log [default: %default]')
    parser.add_option('--machine', type='str', dest='machine', default=socket.gethostname(), help='machine to log [default: %default]')
    parser.add_option('--comment', type='str', dest='comment', default='none', help='some comment to log [default: %default]')
    parser.add_option('--random-forest', type='string', dest='rf_fn', default=None, help='use cellness prediction instead of indicator function for (mis-)detection energy')
    parser.add_option('--ep_gap', type='float', dest='ep_gap', default=0.01, help='stop optimization as soon as a feasible integer solution is found proved to be within the given percent of the optimal solution')
    parser.add_option('-f', '--forbidden_cost', type='float', dest='forb', default=0, help='forbidden cost [default: %default]')
    parser.add_option('--min-ts', type='int', dest='mints', default=0, help='[default: %default]')
    parser.add_option('--max-ts', type='int', dest='maxts', default=-1, help='[default: %default]')
    parser.add_option('--min-size', type='int', dest='minsize', default=0, help='minimal size of objects to be tracked [default: %default]')
    parser.add_option('--dump-traxelstore', type='string', dest='dump_traxelstore', default=None, help='dump traxelstore to file [default: %default]')
    parser.add_option('--load-traxelstore', type='string', dest='load_traxelstore', default=None, help='load traxelstore from file [default: %default]')
    

    chaingraphopts = optparse.OptionGroup(parser, "chaingraph method")
    chaingraphopts.add_option('-p', '--opportunity_cost', type='float', dest='opp', default=0, help='opportunity cost [default: %default]')
    
    chaingraphopts.add_option('--without-hard-constraints', action='store_true', dest='without_constraints', help='without hard constraints')
    chaingraphopts.add_option('--fixed-detections', action='store_true', dest='fixed_detections', help='set all detections to active via hard constraint; not affected by --without-hard-constraints')    
    chaingraphopts.add_option('--mean_div_dist', type='float', dest='mdd', default=25, help='average division distance [default: %default]')
    chaingraphopts.add_option('--min_angle', type='float', dest='ma', default=0, help='minimal division angle [default: %default]')
    chaingraphopts.add_option('-a', '--appearance', type='float', dest='app', default=500, help='appearance cost [default: %default]')
    chaingraphopts.add_option('-d', '--disappearance', type='float', dest='dis', default=500, help='disappearance cost [default: %default]')
    chaingraphopts.add_option('-e', '--detection', type='float', dest='det', default=10, help='detection weight [default: %default]')
    chaingraphopts.add_option('-m', '--misdetection', type='float', dest='mis', default=200, help='misdetection weight [default: %default]')
    chaingraphopts.add_option('-n', '--n-neighbors', type='int', dest='nnc', default=2, help='number of neighbors taken into account when building hypothesesgraph [default: %default]')

    
    consopts = optparse.OptionGroup(parser, "conservation tracking")
    consopts.add_option('--max-number-objects', dest='mno', type='float', default=2, help='Give maximum number of objects one connected component may consist of [default: %default]')
    consopts.add_option('--max-neighbor-distance', dest='mnd', type='float', default=30, help='[default: %default]')
    consopts.add_option('--division-threshold', dest='dt', type='float', default=0.1, help='[default: %default]')
    # detection_rf_filename in general parser options
    consopts.add_option('--size-dependent-detection-prob', dest='sddp', action='store_true')
    # forbidden_cost in general parser options
    # ep_gap in general parser options
    consopts.add_option('--average-obj-size', dest='aos', type='float', default=0, help='[default: %default]')
    consopts.add_option('--without-tracklets', dest='wot', action='store_true')
    consopts.add_option('--with-opt-correct', dest='woptical', action='store_true')
    consopts.add_option('--div', dest='dw', type='float', default=10.0, help='division weight [default: %default]')
    consopts.add_option('--dis', dest='dis', type='float', default=500.0, help='disappearance cost [default: %default]')
    consopts.add_option('--app', dest='app', type='float', default=500.0, help='appearance cost [default: %default]')
    consopts.add_option('--tr', dest='tw', type='float', default=10.0, help='transition weight [default: %default]')
    consopts.add_option('--without-divisions', dest='wod', action='store_true')
    consopts.add_option('--means', dest='means', type='float', default=0.0, help='means for detection [default: %default]')
    consopts.add_option('--sigma', dest='sigma', type='float', default=0.0, help='sigma for detection [default: %default]')
    consopts.add_option('--with-merger-resolution', dest='wmr', action='store_true', default=False)
    consopts.add_option('--without-constraints', dest='woconstr', action='store_true', default=False)
    consopts.add_option('--trans-par', dest='trans_par', type='float', default=5.0, help='alpha for the transition prior [default: %default]')
    consopts.add_option('--border-width', dest='border_width', type='float', default=0.0, help='absolute border margin in which the appearance/disappearance costs are linearly decreased [default: %default]')
    consopts.add_option('--ext-probs', dest='ext_probs', type='string', default=None, help='provide a path to hdf5 files containing detection probabilities [default:%default]')

    consopts.add_option('--objCountPath', dest='obj_count_path', type='string', default='/CellClassification/Probabilities/0/', help='internal hdf5 path to object count probabilities [default=%default]')
    consopts.add_option('--divPath', dest='div_prob_path', type='string', default='/DivisionDetection/Probabilities/0/', help='internal hdf5 path to division probabilities [default=%default]')
    consopts.add_option('--featsPath', dest='feats_path', type='string', default='/ObjectExtraction/RegionFeatures/0/[[%d], [%d]]/Standard Object Features/%s', help='internal hdf5 path to object features [default=%default]')
    consopts.add_option('--translationPath', dest='trans_vector_path', type='str', default='OpticalTranslation/TranslationVectors/0/data', help='internal hdf5 path to translation vectors [default=%default]')
    #consopts.add_option('--labelImgPath', dest='label_im_path', type='str', default='/ObjectExtraction/LabelImage/0/[[%d, 0, 0, 0, 0], [%d, %d, %d, %d, 1]]', help='internal hdf5 path to label image [default=%default]')

    parser.add_option_group(chaingraphopts)
    parser.add_option_group(consopts)
    options, args = parser.parse_args()


    numArgs = len(args)
    if numArgs > 0:
        fns = []
        for arg in args:
            fns.extend(glob.glob(arg))
        fns.sort()
    else:
        parser.print_help()
        sys.exit(1)

    if not path.exists(options.out_dir):
        os.makedirs(options.out_dir)

    ### Copy input data
    """print "Copying input data..."
    fns = []
    for fn in fns:
        out_fn = path.join(options.out_dir, path.basename(fn))

        # test, if fn is a valid h5 file
        try:
            f_in = io.LineageH5(fn, 'r')
        except Exception as e:
            print e
            print "!! Not a valid H5 file: " + fn + " -> skipped"
            continue
        f_in.close()
        del f_in

        if options.full_copy:
            print "-- Copy: " + fn + " -> " + out_fn
            shutil.copy(fn, out_fn)
        else:
            print "-- Link/copy: " + fn + " -> " + out_fn
            with io.LineageH5(out_fn, 'w') as f_out:
                if options.with_rel_linking:
                    link_to = path.relpath(path.abspath(path.dirname(fn)), start=path.abspath(path.dirname(out_fn)))
                else:
                    link_to = path.abspath(path.dirname(fn))
                link_fn = path.join(link_to, path.basename(fn))
                f_out["features"] = h5py.ExternalLink(link_fn, "features")
                f_out["Input Segmentation"] = h5py.ExternalLink(link_fn, "Input Segmentation")
                f_out["Input Raw"] = h5py.ExternalLink(link_fn, "Input Raw")
                hdf
                with io.LineageH5(fn, 'r') as f_in:
                    f_in.copy("objects", f_out)
            del f_out
        
        #store out-filename for later use
        working_fns.append(out_fn)
    working_fns.sort()"""
    print

    ### Do the tracking
    start = time.time()
    
    feature_path='ObjectExtraction/'
    with_div = True
    with_merger_prior = True
    if options.method=='chaingraph':
        feature_path='ObjectExtraction'
        with_merger_prior=True # the chaingraph will use the same random forest for detection/misdetection
    # read all traxels into TraxelStore
    print fns[0]
    time_range=[options.mints, options.maxts]
    if options.maxts == -1:
        time_range = None
    obj_size = [0]
    max_traxel_id_at = []
    with h5py.File(fns[0], 'r') as h5file:
        ndim = 3
        if h5file['ObjectExtraction/LabelImage/0/'].values()[0].shape[3] == 1:
            ndim = 2
        print 'ndim=', ndim
        if options.load_traxelstore:
            print 'loading traxelstore from file'
            import pickle
            with open(options.load_traxelstore, 'rb') as ts_in:
               ts = pickle.load(ts_in)
        else:
            if options.method=='conservation':
               max_num_mer = int(options.mno)
            else:
               max_num_mer = 1
            ts, max_traxel_id_at = generate_traxelstore(h5file=h5file,
                  options=options,
				  feature_path=feature_path,
				  time_range = time_range,
				  x_range = None,
				  y_range = None,
				  z_range = None,
				  size_range = [options.minsize, 10000],
				  x_scale=options.x_scale,
				  y_scale=options.y_scale,
				  z_scale=options.z_scale,
				  with_div=with_div,
				  with_local_centers=False,
				  median_object_size=obj_size,
				  max_traxel_id_at=max_traxel_id_at,
              with_merger_prior=with_merger_prior,
              with_coordinates=(options.method=='conservation' and options.mno > 1 and bool(options.wmr)),
              max_num_mergers = max_num_mer,
              with_optical_correction=bool(options.woptical),
              ext_probs = options.ext_probs
              )

        if options.dump_traxelstore:
             print 'dumping traxelstore to file'
             import pickle
             with open(options.dump_traxelstore, 'wb') as ts_out:
                pickle.dump(ts, ts_out)





	"""for i, fn in enumerate(working_fns):
        print "-- reading Traxels from " + fn
        f = io.LineageH5(fn, 'r', timestep=i)
        f.x_scale = options.x_scale
        f.y_scale = options.y_scale
        f.z_scale = options.z_scale
        traxels = f.cTraxels()
        ts.add_from_Traxels(traxels)
        f.close()
        del f
        print "-> %d traxels read" % len(traxels)"""

    if options.aos != 0:
      obj_size[0] = options.aos

    info = [int(x) for x in ts.bounding_box()]
    t0, t1 = (info[0], info[4])
    print "-> Traxelstore bounding box: " + str(info)

    
    print "Start tracking..."
    if(options.method == "conservation"):
        rf_fn = 'none'
        if options.rf_fn:
            rf_fn = options.rf_fn
        with h5py.File(fns[0], 'r') as h5file:
            [xshape, yshape, zshape] = h5file[options.trans_vector_path].shape[1:4]
        fov = track.FieldOfView(t0, 0, 0, 0, t1, options.x_scale * (xshape-1), options.y_scale * (yshape-1) , options.z_scale * (zshape-1))
        if ndim == 2:
            assert options.z_scale * (zshape-1) == 0, "fov of z must be (0,0) if ndim == 2"

        tracker = track.ConsTracking(int(options.mno), options.mnd, options.dt, rf_fn, bool(options.sddp), options.forb, options.ep_gap, obj_size[0], not bool(options.wot), options.dw, options.tw, not bool(options.wod), options.dis, options.app, options.wmr, ndim, options.trans_par, options.border_width, fov, not bool(options.woconstr))
    elif(options.method == "chaingraph"):
        rf_fn = 'none'
        with_rf = False
        if options.rf_fn:
            rf_fn = options.rf_fn
            with_rf = True
        tracker = track.ChaingraphTracking(rf_fn, options.app, options.dis, options.det, options.mis, with_rf, options.opp, options.forb, not(options.without_constraints), options.fixed_detections, options.mdd, options.ma, options.ep_gap, options.nnc)
    else:
        raise Exception("unknown tracking method: " + options.method)

    events = tracker(ts)

    """print "Saving detections..."
    detections = tracker.detections()
    assert(len(detections) == len(working_fns))
    for i, detections_at in enumerate(detections):
        write_detections(detections_at, working_fns[i])"""

    print "Saving events..."
    print "Length of events " + str(len(events))
    working_fns = [options.out_dir.rstrip('/') + "/%04d.h5" % timestep for timestep in xrange(t0,t1+1)]
    assert(len(events) + 1 == len(working_fns))
    with h5py.File(fns[0], 'r') as src_file:
        # first timestep without tracking
        with io.LineageH5(working_fns[0], 'w') as dest_file:
            print "-- writing empty tracking to base file", working_fns[0]
            #shape = src_file[trans_vector_path].shape
            #li_name = label_im_path % (t0, t0+1, shape[1], shape[2], shape[3])
            #label_img = np.array(src_file[li_name][0,...,0])
            meta = dest_file.create_group('objects/meta')
            #ids = np.unique(label_img)
            m = max_traxel_id_at[0]
            ids = np.asarray(range(m+1))
            ids = ids[ids > 0]
            valid = np.ones(ids.shape)
            meta.create_dataset("id", data=ids, dtype=np.uint32)
            meta.create_dataset("valid", data=valid, dtype=np.uint32)

            # create empty tracking group
            dest_file.create_group('tracking')
            print "-> base file successfully written"

            
        # tracked timesteps
        for i, events_at in enumerate(events):
            with io.LineageH5(working_fns[i+1], 'w') as dest_file:
                # [t0+1+i,...] or [t0+i,...]?
                #shape = src_file[trans_vector_path].shape
                #li_name = label_im_path % (t0+i+1, t0+i+2, shape[1], shape[2], shape[3])
                #label_img = np.array(src_file[li_name][0,...,0])

                meta = dest_file.create_group('objects/meta')
                m = max_traxel_id_at[i+1]
                ids = np.asarray(range(m+1))
                #ids = np.unique(label_img)
                ids = ids[ids > 0]
                valid = np.ones(ids.shape)
                meta.create_dataset("id", data=ids[::-1], dtype=np.uint32)
                meta.create_dataset("valid", data=valid, dtype=np.uint32)

            write_events(events_at, working_fns[i + 1])
		
		


    stop = time.time()
    since = stop - start
    print "Elapsed time [s]: " + str(int(since))
    print "Elapsed time [min]: " + str(int(since) / 60)
    print "Elapsed time [h]: " + str(int(since) / 3600)
